<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE rfc SYSTEM "rfc2629.dtd">
<?xml-stylesheet type="text/xsl" href="rfc2629.xslt"?>

<?rfc strict="yes"?>
<?rfc toc="yes"?>
<?rfc symrefs="yes"?>
<?rfc sortrefs="yes"?>
<?rfc compact="yes"?>
<?rfc subcompact="no"?>

<rfc
 docName="draft-ietf-nfsv4-rpcrdma-version-two-latest"
 category="std"
 ipr="pre5378Trust200902">

<front>

<title abbrev="RPC-over-RDMA Version 2">
RPC-over-RDMA Version 2 Protocol
</title>

<author initials="C.L." surname="Lever" fullname="Charles Lever" role="editor">
<organization abbrev="Oracle">Oracle Corporation</organization>
<address>
<postal>
<street></street>
<city></city>
<region></region>
<code></code>
<country>United States of America</country>
</postal>
<email>chuck.lever@oracle.com</email>
</address>
</author>

<author initials="D.N." surname="Noveck" fullname="David Noveck">
<organization>NetApp</organization>
<address>
<postal>
<street>1601 Trapelo Road</street>
<city>Waltham</city>
<region>MA</region>
<code>02451</code>
<country>United States of America</country>
</postal>
<phone>+1 781 572 8038</phone>
<email>davenoveck@gmail.com</email>
</address>
</author>

<date />

<area>Transport</area>
<workgroup>Network File System Version 4</workgroup>
<keyword>NFS-Over-RDMA</keyword>

<abstract>
<t>
This document specifies the second version of a protocol
that conveys Remote Procedure Call (RPC) messages
on transports capable of Remote Direct Memory Access (RDMA).
This version of the protocol is extensible.
</t>
</abstract>

</front>

<middle>

<section
 anchor="section:72f6ba4a-aafb-4e9d-8b87-800ebccc5879"
 title="Introduction">
<t>
Remote Direct Memory Access (RDMA)
<xref target="RFC5040"/>
<xref target="RFC5041"/>
<xref target="IBA"/>
is a technique for moving data efficiently between network nodes.
By directing data into destination buffers as it is sent
on a network and placing it using direct memory access
implemented by hardware,
the complementary benefits of
faster transfers
and
reduced host overhead
are obtained.
</t>
<t>
Open Network Computing Remote Procedure Call
(ONC RPC, often shortened in NFSv4 documents to RPC)
<xref target="RFC5531"/>
is a Remote Procedure Call protocol
that runs over a variety of transports.
Most RPC implementations today use UDP
<xref target="RFC0768"/>
or
TCP
<xref target="RFC0793"/>.
On UDP, RPC messages are encapsulated inside datagrams,
while on a TCP byte stream,
RPC messages are delineated by a record marking protocol.
An RDMA transport also conveys RPC messages
in a specific fashion that must be fully described
if RPC implementations are to interoperate
when using RDMA to transport RPC transactions.
</t>
<t>
RDMA transports present semantics that differ from either UDP or TCP.
They retain message delineations like UDP
but provide reliable and sequenced data transfer like TCP.
They also provide an offloaded bulk transfer service
not provided by UDP or TCP.
RDMA transports are therefore appropriately treated
as a new transport type by RPC.
</t>
<t>
Although the RDMA transport described herein
can provide relatively transparent support for any RPC application,
this document also describes mechanisms that enable further optimization of data transfer,
when RPC applications are structured to exploit awareness of a transport's RDMA capability.
In this context, the Network File System (NFS) protocols,
as described in
<xref target="RFC1094"/>,
<xref target="RFC1813"/>,
<xref target="RFC7530"/>,
<xref target="RFC5661"/>,
and subsequent NFSv4 minor versions,
are all potential beneficiaries of RDMA transports.
A complete problem statement is presented in
<xref target="RFC5532"/>.
</t>
<t>
The RPC-over-RDMA version 1 protocol specified in
<xref target="RFC8166"/>
is deployed and in use,
although there are known shortcomings to this protocol:
<list style="symbols">
<t>
The protocol's default size of Receive buffers forces
the use of RDMA Read and Write transfers for small payloads,
and limits the size of reverse direction messages.
</t>
<t>
It is difficult to make optimizations or protocol fixes
that require changes to on-the-wire behavior.
</t>
<t>
For some RPC procedures, the maximum reply size is
difficult or impossible for an RPC client to estimate
in advance.
</t>
</list>
To address these issues in a way that enables interoperation
with existing RPC-over-RDMA version 1 deployments,
a second version of the RPC-over-RDMA transport protocol
is presented in this document.
</t>
<t>
Version 2 of RPC-over-RDMA is extensible,
enabling OPTIONAL extensions to be added
without impacting existing implementations.
To enable protocol extension,
the XDR definition for RPC-over-RDMA version 2 is
organized differently than the definition version 1.
These changes, which are discussed in
<xref target="section:d945b9f0-0666-4db7-9126-be57cf7b5f4f"/>,
do not alter the on-the-wire format.
</t>
<t>
In addition, RPC-over-RDMA version 2 contains
a set of incremental changes that relieve certain
performance constraints and enable recovery from
abnormal corner cases.
These changes are outlined in
<xref target="section:c2574344-5aec-427d-a5ed-048d7fcc0d95"/>
and include
a larger default inline threshold,
the ability to convey a single RPC message using multiple RDMA Send operations,
support for authentication of connection peers,
richer error reporting,
an improved credit-based flow control mechanism,
and
support for Remote Invalidation.
</t>
</section>

<section
 anchor="section:ef1a2819-4d22-40af-8d38-fde10849c872"
 title="Requirements Language">
<t>
The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT RECOMMENDED", "MAY",
and "OPTIONAL" in this document are to be interpreted
as described in BCP 14
<xref target="RFC2119"/>
<xref target="RFC8174"/>
when, and only when, they appear in all capitals, as shown here.
</t>
</section>

<section
 anchor="section:4dc39c9c-3770-491f-b674-f824e87e2143"
 title="Terminology">

<section
 anchor="section:4d675e45-c377-48df-8029-c5c9f8c48f9f"
 title="Remote Procedure Calls">
<t>
This section highlights key elements of the RPC protocol
<xref target="RFC5531"/>
and
the External Data Representation (XDR)
<xref target="RFC4506"/>
used by it.
RPC-over-RDMA version 2 enables the transmission of RPC messges built using XDR
and also uses XDR internaly to describe its own header formats.
An understanding of RPC and its use of XDR is assumed in this document.
</t>

<section
 anchor="section:8d804fe5-c7c7-4c6c-92d8-888da10caaec"
 title="Upper-Layer Protocols">
<t>
RPCs are an abstraction used to implement the operations of an Upper-Layer Protocol (ULP).
"ULP" refers to an RPC Program and Version tuple,
which is a versioned set of procedure calls that comprise a single well-defined API.
One example of a ULP is the Network File System Version 4.0
<xref target="RFC7530"/>.
</t>
<t>
In this document, the term "RPC consumer" refers to
an implementation of a ULP running on an RPC client.
</t>
</section>

<section
 anchor="section:17a77782-8b11-4fb5-af0b-e0da7759c10A"
 title="Requesters and Responders">
<t>
Like a local procedure call,
every RPC procedure has a set of "arguments" and a set of "results".
A calling context invokes a procedure,
passing arguments to it,
and the procedure subsequently returns a set of results.
Unlike a local procedure call,
the called procedure is executed remotely rather than
in the local application's execution context.
</t>
<t>
The RPC protocol as described in
<xref target="RFC5531"/>
is fundamentally a message-passing protocol
between one or more clients, where RPC consumers are running,
and a server, where a remote execution context is available
to process RPC transactions on behalf of those consumers.
</t>
<t>
ONC RPC transactions are made up of two types of messages:
<list style="hanging">
<t hangText="CALL"><vspace blankLines="0"/>
A CALL message, or "Call",
requests that work be done.
An RPC Call message is designated
by the value zero (0) in the message's msg_type field.
An arbitrary unique value is placed in the message's XID field
in order to match this RPC Call message to a corresponding
RPC Reply message.
</t>
<t hangText="REPLY"><vspace blankLines="0"/>
A REPLY message, or "Reply",
reports the results of work requested by an RPC Call message.
An RPC Reply message is designated
by the value one (1) in the message's msg_type field.
The value contained in an RPC Reply message's XID field
is copied from the RPC Call message whose results are being reported.
</t>
</list>
</t>
<t>
Each RPC client endpoint acts as a "Requester".
It serializes the procedure's arguments
and
conveys them to a server endpoint via an RPC Call message.
This message contains an RPC protocol header,
a header describing the requested upper-layer operation,
and all arguments.
</t>
<t>
An RPC server endpoint acts as a "Responder".
It deserializes the arguments and processes the requested operation.
It then serializes the operation's results into another byte stream.
This byte stream is conveyed back to the Requester
via an RPC Reply message.
This message contains an RPC protocol header,
a header describing the upper-layer reply,
and all results.
</t>
<t>
The Requester deserializes the results
and
allows the RPC consumer to proceed.
At this point, the RPC transaction
designated by the XID in the RPC Call message is complete,
and the XID is retired.
</t>
<t>
In summary,
Requesters send RPC Call messages to Responders
to initiate RPC transactions.
Responders send RPC Reply messages to Requesters
to complete the processing on an RPC transaction.
</t>
</section>

<section
 anchor="section:63f47fcd-629b-4b00-aa8b-dbf836401581"
 title="RPC Transports">
<t>
The role of an "RPC transport" is to
mediate the exchange of RPC messages between Requesters and Responders.
An RPC transport bridges the gap between
the RPC message abstraction
and
the native operations of a particular network transport.
</t>
<t>
RPC-over-RDMA is a connection-oriented RPC transport.
When a connection-oriented transport is used,
clients initiate transport connections,
while servers wait passively to accept incoming connection requests.
</t>
<t>
Most commonly,
the client end of the connection acts in the role of Requester,
and the server end of the connection acts as a Responder.
However,
RPC transactions can also be sent in the reverse direction.
In this case, the server end of the connection acts as a Requestor
while the client end acts as a Responder.
</t>
</section>

<section
 anchor="section:98bdc62c-0af4-4379-8b5c-6d98b7a520c7"
 title="External Data Representation">
<t>
One cannot assume that all Requesters and Responders
represent data objects the same way internally.
RPC uses External Data Representation (XDR)
to translate native data types and serialize arguments and results
<xref target="RFC4506"/>.
</t>
<t>
The XDR protocol encodes data independently
of the endianness or size of host-native data types,
enabling unambiguous decoding of data by the receiver.
RPC Programs are specified
by writing an XDR definition of their procedures,
argument data types,
and
result data types.
</t>
<t>
XDR assumes only that the number of bits in a byte (octet)
and
their order are the same on both endpoints and on the physical network.
The smallest indivisible unit of XDR encoding is a group of four octets.
XDR can also flatten
lists,
arrays,
and
other complex data types
so they can be conveyed as a stream of bytes.
</t>
<t>
A serialized stream of bytes
that is the result of XDR encoding
is referred to as an "XDR stream".
A sending endpoint encodes native data
into an XDR stream and then transmits that stream to a receiver.
A receiving endpoint decodes incoming XDR byte streams
into its native data representation format.
</t>

<section
 anchor="section:c6d3092c-99e6-4cce-b377-fffc4862929F"
 title="XDR Opaque Data">
<t>
Sometimes, a data item is to be transferred as is:
without encoding or decoding.
The contents of such a data item are referred to as "opaque data".
XDR encoding places the content of opaque data items
directly into an XDR stream without altering it in any way.
ULPs or applications perform
any needed data translation in this case.
Examples of opaque data items include the content of files
or generic byte strings.
</t>
</section>

<section
 anchor="section:c210323f-c524-4e98-a02d-23549a4bebc5"
 title="XDR Roundup">
<t>
The number of octets in a variable-length data item
precedes that item in an XDR stream.
If the size of an encoded data item is not a multiple of four octets,
octets containing zero are added after the end of the item.
This is the case so that the next encoded data item in the XDR stream
always starts on a four-octet boundary.
The encoded size of the item is not changed
by the addition of the extra octets.
These extra octets are never exposed to ULPs.
</t>
<t>
This technique is referred to as "XDR roundup",
and the extra octets are referred to as "XDR roundup padding".
</t>
</section>

</section>

</section>

<section
 anchor="section:de830270-64ed-4510-ac25-29837d352031"
 title="Remote Direct Memory Access">
<t>
RPC Requesters and Responders can be made more efficient
if large RPC messages are transferred by a third party,
such as intelligent network-interface hardware
(data movement offload),
and placed in the receiver's memory so that
no additional adjustment of data alignment has to be made
(direct data placement or "DDP").
RDMA transports enable both optimizations.
</t>
<t>
In the current document, "RDMA" refers to
the physical mechanism an RDMA transport utilizes when moving data.
</t>

<section
 anchor="section:1b97ecfd-7aba-4299-9007-dab28ac76f81"
 title="Direct Data Placement">
<t>
Typically, RPC implementations copy
the contents of RPC messages into a buffer before being sent.
An efficient RPC implementation sends bulk data
without copying it into a separate send buffer first.
</t>
<t>
However, socket-based RPC implementations
are often unable to receive data directly
into its final place in memory.
Receivers often need to copy incoming data
to finish an RPC operation:
sometimes, only to adjust data alignment.
</t>
<t>
Although it may not be efficient,
before an RDMA transfer, a sender may copy data into an intermediate buffer.
After an RDMA transfer, a receiver may copy that data again to its final destination.
In this document, the term "DDP" refers to
any optimized data transfer where it is unnecessary
for a receiving host's CPU to copy transferred data
to another location after it has been received.
</t>
<t>
RPC-over-RDMA version 2 enables the use of RDMA Read and Write operations
to achieve both data movement offload and DDP.
However, not all RDMA-based data transfer qualifies as DDP,
and DDP can be achieved using non-RDMA mechanisms.
</t>
</section>

<section
 anchor="section:6903045e-bd1c-4e12-bf96-6b534989f46A"
 title="RDMA Transport Requirements">
<t>
To achieve good performance during receive operations,
RDMA transports require that
RDMA consumers provision resources in advance
in order to receive incoming messages.
</t>
<t>
An RDMA consumer might provide Receive buffers in advance
by posting an RDMA Receive Work Request
for every expected RDMA Send from a remote peer.
These buffers are provided
before the remote peer posts RDMA Send Work Requests.
Thus this is often referred to as "pre-posting" buffers.
</t>
<t>
An RDMA Receive Work Request remains outstanding
until hardware matches it to an inbound Send operation.
The resources associated with that Receive must be retained in
host memory, or "pinned", until the Receive completes.
</t>
<t>
Given these basic tenets of RDMA transport operation,
the RPC-over-RDMA version 2 protocol assumes
each transport provides the following abstract operations.
A more complete discussion of these operations can be found in
<xref target="RFC5040"/>.
</t>

<section
 anchor="section:90f88ba5-5ad6-4ac1-b40d-ed9247e61ca5"
 title="Memory Registration">
<t>
Memory registration assigns a steering tag
to a region of memory,
permitting the RDMA provider
to perform data-transfer operations.
The RPC-over-RDMA version 2 protocol assumes that
each registered memory region is identified
with a steering tag of no more than 32 bits and memory
addresses of up to 64 bits in length.
</t>
</section>

<section
 anchor="section:07bba55f-c48f-474c-918b-db6c9d2325dd"
 title="RDMA Send">
<t>
The RDMA provider supports an RDMA Send operation,
with completion signaled on the receiving peer
after data has been placed in a pre-posted buffer.
Sends complete at the receiver
in the order they were issued at the sender.
The amount of data transferred by a single RDMA Send operation
is limited by the size of the remote peer's pre-posted buffers.
</t>
</section>

<section
 anchor="section:9be6a44c-1ea5-4ccd-b188-ee04e930497b"
 title="RDMA Receive">
<t>
The RDMA provider supports an RDMA Receive operation
to receive data conveyed by incoming RDMA Send operations.
To reduce the amount of memory that must remain pinned
awaiting incoming Sends,
the amount of pre-posted memory is limited.
Flow control to prevent overrunning receiver resources
is provided by the RDMA consumer
(in this case, the RPC-over-RDMA version 2 protocol).
</t>
</section>

<section
 anchor="section:cfd79bce-e8e9-4a51-b43a-b747af6213f4"
 title="RDMA Write">
<t>
The RDMA provider supports an RDMA Write operation
to place data directly into a remote memory region.
The local host initiates an RDMA Write,
and completion is signaled there.
No completion is signaled on the remote peer.
The local host provides
a steering tag,
memory address,
and
the length of the remote peer's memory region.
</t>
<t>
RDMA Writes are not ordered with respect to one another,
but are ordered with respect to RDMA Sends.
A subsequent RDMA Send completion
obtained at the write initiator
guarantees that prior RDMA Write data
has been successfully placed in the remote peer's memory.
</t>
</section>

<section
 anchor="section:f37121af-49ff-4575-a699-7310f4ae1296"
 title="RDMA Read">
<t>
The RDMA provider supports an RDMA Read operation
to place peer source data directly into the read initiator's memory.
The local host initiates an RDMA Read,
and completion is signaled there.
No completion is signaled on the remote peer.
The local host provides
steering tags,
memory addresses,
and a length for the remote source
and
local destination memory region.
</t>
<t>
The local host signals Read completion to the remote peer
as part of a subsequent RDMA Send message.
The remote peer can then invalidate steering tags
and
subsequently free associated source memory regions.
</t>
</section>

</section>

</section>

</section>

<section
 anchor="section:5ae4b016-9b44-4649-9021-5ae851ac9326"
 title="RPC-over-RDMA Protocol Framework">

<section
 anchor="section:27a8a236-70bd-4ded-9272-c65f3a5e4067"
 title="Transfer Model">
<t>
A "transfer model" designates which endpoint exposes its memory
and which is responsible for initiating the transfer of data.
To enable RDMA Read and Write operations, for example,
an endpoint first exposes regions of its memory to a remote endpoint,
which initiates these operations against the exposed memory.
</t>
<t>
In RPC-over-RDMA version 2,
Requesters expose their memory to the Responder,
but the Responder does not expose its memory.
The Responder pulls RPC arguments
or whole RPC calls from each Requester.
The Responder pushes RPC results
or whole RPC replies to each Requester.
</t>
</section>

<section
 anchor="section:195e0288-862d-40bb-a259-4239930c728a"
 title="Message Framing">
<t>
Each RPC-over-RDMA version 2 message consists of at most two XDR streams:
<list style="hanging">
<t hangText="Transport Stream"><vspace blankLines="0"/>
The "Transport stream" contains a header that describes
and controls the transfer of the Payload stream
in this RPC-over-RDMA message.
Every RDMA Send message on an RPC-over-RDMA version 2
connection MUST begin with a Transport stream.
</t>
<t hangText="RPC Payload Stream"><vspace blankLines="0"/>
The "Payload stream" contains part or all of a single RPC message.
The sender
MAY divide an RPC message at any convenient boundary,
but
MUST send RPC message fragments in XDR stream order
and
MUST NOT interleave Payload streams from multiple RPC messages.
The RPC-over-RDMA version 2 message carrying the final part
of an RPC message is marked (see
<xref target="section:fb299673-74f3-4f01-adb5-26a02ccd679f"/>).
</t>
</list>
In its simplest form, an RPC-over-RDMA version 2 message
conveying an RPC message payload consists of
a Transport stream
followed immediately by
a Payload stream
transmitted together via a single RDMA Send.
</t>
<t>
RPC-over-RDMA framing replaces all other RPC framing
(such as TCP record marking)
when used atop an RPC-over-RDMA association,
even when the underlying RDMA protocol may itself
be layered atop a transport with a defined RPC framing (such as TCP).
</t>
<t>
However, it is possible for RPC-over-RDMA to be dynamically enabled
on a connection
in the course of negotiating the use of RDMA via a ULP exchange.
Because RPC framing delimits an entire RPC request or reply,
the resulting shift in framing must occur between distinct RPC messages,
and in concert with the underlying transport.
</t>
</section>

<section
 anchor="section:130ce79c-8b13-479e-8108-a943024047dD"
 title="Managing Receiver Resources">
<t>
The longevity of an RDMA connection mandates
that sending endpoints respect the resource limits of peer receivers.
To ensure messages can be sent and received reliably,
there are two operational parameters for each connection.
It is critical to provide RDMA Send flow control
for an RDMA connection.
If any pre-posted Receive buffer on the connection is not large enough
to accept an incoming RDMA Send,
or if a pre-posted Receive buffer is not available to accept an incoming RDMA Send,
the RDMA connection can be terminated.
</t>

<section
 anchor="section:45c67eb8-8dc6-47c3-8555-14270f1514bF"
 title="RPC-over-RDMA Version 2 Flow Control">
<t>
Because RPC-over-RDMA requires reliable and in-order delivery
of data payloads,
RPC-over-RDMA transports MUST use
the RDMA RC (Reliable Connected) Queue Pair (QP) type,
which ensures in-transit data integrity
and
handles recovery from packet loss or misordering.
</t>
<t>
However, RPC-over-RDMA transports provide their own
flow control mechanism to prevent a sender
from overwhelming receiver resources.
RPC-over-RDMA transports employ
an end-to-end credit-based flow control mechanism
for this purpose
<xref target="CBFC"/>.
Credit-based flow control was chosen because it is
relatively simple,
provides robust operation in the face of bursty traffic,
automated management of receive buffer allocation,
and
excellent buffer utilization.
</t>

<section
 anchor="section:278fca6a-f2a8-42f3-a4e9-a8d4b217567e"
 title="Granting Credits">
<t>
An RPC-over-RDMA version 2 credit is
the capability to receive one RPC-over-RDMA version 2 message.
This enables RPC-over-RDMA version 2 to support asymmetrical operation,
where a message in one direction might be matched by zero, one, or multiple
messages in the other direction.
</t>
<t>
To achieve this,
credits are assigned to each connection peer's posted Receive buffers.
Each Requester has a set of Receive credits, and
each Responder has a set of Receive credits.
These credit values are managed independently of one another.
</t>
<t>
Section 7 of
<xref target="RFC8166"/>
requires that the 32-bit field containing the credit grant is the
third word in the transport header.
To conform with that requirement,
the two independent credit values are encoded into a single
32-bit field in the fixed portion of the transport header.
After the field is XDR decoded, the receiver takes the low-order
two bytes as the number of credits that are newly granted by the sender,
and
the high-order two bytes as the maximum number of credits that
can be outstanding at the sender.
</t>
<t>
In this approach, then, there are requester credits,
sent in messages from the requester to the responder;
and responder credits,
sent in messages from the responder to the requester.
</t>
<t>
A sender MUST NOT send RDMA messages
in excess of the receiver's granted credit limit.
If the granted value is exceeded,
the RDMA layer may signal an error,
possibly terminating the connection.
The granted value MUST NOT be zero,
since such a value would result in deadlock.
</t>
<t>
The granted credit values MAY be adjusted
to match the needs or policies in effect on either peer.
For instance, a peer may reduce its granted credit
value to accommodate the available resources in a Shared Receive Queue.
</t>
<t>
Certain RDMA implementations may impose additional flow-control restrictions,
such as limits on RDMA Read operations in progress at the Responder.
Accommodation of such restrictions is considered
the responsibility of each RPC-over-RDMA version 2 implementation.
</t>
</section>

<section
 anchor="section:f6348562-97f8-4413-96d7-bde3ff57b375"
 title="Asynchronous Credit Grants">
<t>
A protocol convention is provided to enable one peer
to refresh its credit grant to the other peer
without sending a data payload.
Messages of this type can also act as a keep-alive ping.
See
<xref target="section:1c401555-4b7d-4e35-a9a3-8aa14228170e"/>
for information about this convention.
</t>
<t>
To prevent transport deadlock,
receivers MUST always be in a position to receive one such
credit grant update message, in addition to payload-bearing messages.
One way a receiver can do this is to post one extra Receive more than
the credit value it granted.
</t>
</section>

</section>

<section
 anchor="section:653e7ab4-782f-43f2-947f-d097ada8b3c9"
 title="Inline Threshold">
<t>
An "inline threshold" value is the largest message size
(in octets) that can be conveyed in one direction between
peer implementations using RDMA Send and Receive operations.
The inline threshold value is effectively
the smaller of the largest number of bytes the sender can post via a single RDMA Send operation
and
the largest number of bytes the receiver can accept via a single RDMA Receive operation.
Each connection has two inline threshold values:
one for messages flowing from Requester-to-Responder,
referred to as the "call inline threshold",
and
one for messages flowing from Responder-to-Requester,
referred to as the "reply inline threshold".
Inline threshold values can be advertised to peers
via Transport Properties.
</t>
<t>
Receiver implementations MUST support inline thresholds of 4096 bytes.
In the absence of an exchange of Transport Properties,
senders and receivers MUST assume both connection inline thresholds are 4096 bytes.
</t>
</section>

<section
 anchor="section:9563f1f0-28a7-48e1-a752-e842575c6539"
 title="Initial Connection State">
<t>
When an RPC-over-RDMA version 2 client establishes a
connection to a server, its first order of business is to
determine the server's highest supported protocol version.
</t>
<t>
Upon connection establishment a client MUST NOT send
more than a single RPC-over-RDMA message at a
time until it receives a valid non-error RPC-over-RDMA message
from the server that grants client credits.
</t>
<t>
The second word of each transport header is used to convey
the transport protocol version.
In the interest of simplicity, we refer to that word as
rdma_vers even though in the RPC-over-RDMA version 2
XDR definition it is described as rdma_start.rdma_vers.
</t>
<t>
First, the client sends a single valid RPC-over-RDMA message
with the value two (2) in the rdma_vers field.
Because the server might support only RPC-over-RDMA
version 1, this initial message MUST NOT be larger than the
version 1 default inline threshold of 1024 bytes.
</t>

<section
 anchor="section:8db4c54e-c1ce-43ba-93b4-031e829960f5"
 title="Server Does Support RPC-over-RDMA Version 2">
<t>
If the server does support RPC-over-RDMA version 2,
it sends RPC-over-RDMA messages back to the client
with the value two (2) in the rdma_vers field.
Both peers may use the default inline threshold value
for RPC-over-RDMA version 2 connections (4096 bytes).
</t>
</section>

<section
 anchor="section:bedc4e66-4295-4dd6-8ac9-dd06907a08ad"
 title="Server Does Not Support RPC-over-RDMA Version 2">
<t>
If the server does not support RPC-over-RDMA version 2,
it MUST send an RPC-over-RDMA message to the client with the
same XID, with RDMA2_ERROR in the rdma_start.rdma_htype field,
and with the error code RDMA2_ERR_VERS.
This message also reports a range of protocol versions that
the server supports.
To continue operation, the client selects a protocol
version in the range of server-supported versions for
subsequent messages on this connection.
</t>
<t>
If the connection is lost immediately after an
RDMA2_ERROR / RDMA2_ERR_VERS message is received,
a client can avoid a possible version negotiation loop
when re-establishing another connection by assuming
that particular server does not support RPC-over-RDMA version 2.
A client can assume the same situation (no server support
for RPC-over-RDMA version 2) if the initial negotiation message
is lost or dropped.
Once the negotiation exchange is complete,
both peers may use the default inline threshold value
for the transport protocol version that has been selected.
</t>
</section>

<section
 anchor="section:1ffe4c69-b516-476a-bba7-41863709f48d"
 title="Client Does Not Support RPC-over-RDMA Version 2">
<t>
If the server supports the RPC-over-RDMA protocol version
used in the first RPC-over-RDMA message received from a client,
it MUST use that protocol version in all subsequent messages
it sends on that connection.
The client MUST NOT change the protocol version
for the duration of the connection.
</t>
</section>

</section>

</section>

<section
 anchor="section:cfa8877c-b905-455d-b420-bf7a4a7f7829"
 title="XDR Encoding with Chunks">
<t>
When a DDP capability is available,
the transport places the contents of one or more XDR data items
directly into the receiver's memory,
separately from the transfer of other parts of the containing XDR stream.
</t>

<section
 anchor="section:0e225040-d15f-4e9a-a0c3-afa115bb12c7"
 title="Reducing an XDR Stream">
<t>
RPC-over-RDMA version 2 provides a mechanism for
moving part of an RPC message via a data transfer distinct
from an RDMA Send/Receive pair.
The sender removes one or more XDR data items from the Payload stream.
These items are conveyed via other mechanisms,
such as one or more RDMA Read or Write operations.
As the receiver decodes an incoming message,
it skips over directly placed data items.
</t>
<t>
The portion of an XDR stream that is split out and moved separately
is referred to as a "chunk".
In some contexts,
data in an RPC-over-RDMA header that describes these split out regions of memory
may also be referred to as a "chunk".
</t>
<t>
A Payload stream after chunks have been removed
is referred to as a "reduced" Payload stream.
Likewise, a data item that has been removed
from a Payload stream to be transferred separately
is referred to as a "reduced" data item.
</t>
</section>

<section
 anchor="section:155d7e5e-821b-485a-9b68-1248b7e35743"
 title="DDP-Eligibility">
<t>
Not all XDR data items benefit from DDP.
For example, small data items
or
data items that require XDR unmarshaling by the receiver
do not benefit from DDP.
In addition, it is impractical for receivers to prepare
for every possible XDR data item in a protocol to be transferred in a chunk.
</t>
<t>
To maintain practical interoperability on an RPC-over-RDMA transport,
a determination must be made of which few XDR data items in each ULP
are allowed to use DDP.
</t>
<t>
This is done in additional specifications that describe how ULPs employ DDP.
A "ULB specification" identifies which specific individual XDR data items
in a ULP MAY be transferred via DDP.
Such data items are referred to as "DDP-eligible".
All other XDR data items MUST NOT be reduced.
Detailed requirements for ULBs are provided in
<xref target="section:9e003b83-66b5-43d7-b9ef-0f271c8d301b"/>.
</t>
</section>

<section
 anchor="section:8966c401-1714-413c-9384-b1f71f0a920d"
 title="RDMA Segments">
<t>
When encoding a Payload stream that contains a DDP-eligible data item,
a sender may choose to reduce that data item.
When it chooses to do so, the sender does not place the item into the
Payload stream.
Instead, the sender records in the RPC-over-RDMA Transport header
the location and size of the memory region containing that data item.
</t>
<t>
The Requester provides location information for DDP-eligible data items
in both RPC Call and Reply messages.
The Responder uses this information
to retrieve arguments contained in the specified region of the Requester's memory
or
place results in that memory region.
</t>
<t>
An "RDMA segment", or "plain segment",
is an RPC-over-RDMA Transport header data object
that contains the precise coordinates of a contiguous memory region
that is to be conveyed separately from the Payload stream.
Plain segments contain the following information:
<list style="hanging">
<t hangText="Handle"><vspace blankLines="0"/>
Steering tag (STag) or R_key generated by registering this memory with the RDMA provider.
</t>
<t hangText="Length"><vspace blankLines="0"/>
The length of the RDMA segment's memory region, in octets.
An "empty segment" is an RDMA segment
with the value zero (0) in its length field.
</t>
<t hangText="Offset"><vspace blankLines="0"/>
The offset or beginning memory address of the RDMA segment's memory region.
</t>
</list>
See
<xref target="RFC5040"/>
for further discussion.
 </t>
</section>

<section
 anchor="section:c561e45c-fe88-47e3-bdfd-689d482fcad3"
 title="Chunks">
<t>
In RPC-over-RDMA version 2,
a "chunk" refers to a portion of the Payload stream
that is moved independently
of the RPC-over-RDMA Transport header and Payload stream.
Chunk data is removed from the sender's Payload stream,
transferred via separate operations,
and then reinserted into the receiver's Payload stream
to form a complete RPC message.
</t>
<t>
Each chunk is comprised of RDMA segments.
Each RDMA segment represents a single contiguous piece of that chunk.
A Requester MAY divide a chunk into RDMA segments
using any boundaries that are convenient.
The length of a chunk is exactly
the sum of the lengths of the RDMA segments that comprise it.
</t>
<t>
The RPC-over-RDMA version 2 transport protocol
does not place a limit on chunk size.
However, each ULP may cap the amount of data that can be transferred
by a single RPC transaction.
For example, NFS has "rsize" and "wsize", which restrict the payload size
of NFS READ and WRITE operations.
The Responder can use such limits to sanity check chunk sizes
before using them in RDMA operations.
</t>

<section
 anchor="section:2b8d1299-b521-4198-b99c-25b4e344b8bb"
 title="Counted Arrays">
<t>
If a chunk contains a counted array data type,
the count of array elements MUST remain in the Payload stream,
while the array elements MUST be moved to the chunk.
For example, when encoding an opaque byte array as a chunk,
the count of bytes stays in the Payload stream,
while the bytes in the array are removed from the Payload stream
and transferred within the chunk.
</t>
<t>
Individual array elements appear in a chunk
in their entirety.
For example, when encoding an array of arrays as a chunk,
the count of items in the enclosing array stays in the Payload stream,
but each enclosed array, including its item count,
is transferred as part of the chunk.
</t>
</section>

<section
 anchor="section:ec322087-6d38-42b9-b24b-831aabcfb5f9"
 title="Optional-Data">
<t>
If a chunk contains an optional-data data type,
the "is present" field MUST remain in the Payload stream,
while the data, if present, MUST be moved to the chunk.
</t>
</section>

<section
 anchor="section:5ae887e6-fd9d-4a9f-a298-925b87a6a5b2"
 title="XDR Unions">
<t>
A union data type MUST NOT be made DDP-eligible,
but one or more of its arms MAY be DDP-eligible,
subject to the other requirements in this section.
</t>
</section>

<section
 anchor="section:a7aa6a3a-2d02-4c11-bb88-0ba2eb76b0c0"
 title="Chunk Roundup">
<t>
Except in special cases (covered in
<xref target="section:d9731520-bdde-4a1b-9f54-9901d5c57648"/>),
a chunk MUST contain exactly one XDR data item.
This makes it straightforward to reduce variable-length data items
without affecting the XDR alignment of data items in the Payload stream.
</t>
<t>
When a variable-length XDR data item is reduced,
the sender MUST remove XDR roundup padding for that data item
from the Payload stream so that data items remaining
in the Payload stream begin on four-byte alignment.
</t>
</section>

</section>

<section
 anchor="section:025098f1-1355-4c97-8fda-5b4859372aa5"
 title="Read Chunks">
<t>
A "Read chunk" represents an XDR data item
that is to be pulled from the Requester to the Responder.
A Read chunk is a list of one or more RDMA read segments.
Each RDMA read segment consists of a Position field followed by a plain segment.
<list style="hanging">
<t hangText="Position"><vspace blankLines="0"/>
The byte offset in the unreduced Payload stream
where the receiver reinserts the data item conveyed in a chunk.
The Position value MUST be computed
from the beginning of the unreduced Payload stream,
which begins at Position zero.
All RDMA read segments belonging to the same Read chunk have the same value
in their Position field.
</t>
</list>
</t>
<t>
While constructing an RPC Call message,
a Requester registers memory regions that contain data to be transferred
via RDMA Read operations.
It advertises the coordinates of these regions
in the RPC-over-RDMA Transport header of the RPC Call message.
</t>
<t>
After receiving an RPC Call message sent via an RDMA Send operation,
a Responder transfers the chunk data from the Requester using RDMA Read operations.
The Responder reconstructs the transferred chunk data
by concatenating the contents of each RDMA segment in list order
into the received Payload stream at the Position value recorded in
that RDMA segment.
</t>
<t>
Put another way, the Responder inserts the first RDMA segment
in a Read chunk into the Payload stream at the byte offset indicated
by its Position field.
RDMA segments whose Position field value match this offset
are concatenated afterwards,
until there are no more RDMA segments at that Position value.
</t>
<t>
The Position field in a read segment
indicates where the containing Read chunk starts in the Payload stream.
The value in this field MUST be a multiple of four.
All segments in the same Read chunk share the same Position value,
even if one or more of the RDMA segments have a non-four-byte-aligned length.
</t>

<section
 anchor="section:5f4cc5f7-325d-479c-b592-6e93979431d0"
 title="Decoding Read Chunks">
<t>
While decoding a received Payload stream,
whenever the XDR offset in the Payload stream
matches that of a Read chunk,
the Responder initiates an RDMA Read to pull the chunk's data content
into registered local memory.
</t>
<t>
The Responder acknowledges its completion of use of Read chunk source buffers
when it sends an RPC Reply message to the Requester.
The Requester may then release Read chunks advertised in the request.
</t>
</section>

<section
 anchor="section:2d24ecfa-6471-43bc-9635-872a07dbe2f8"
 title="Read Chunk Roundup">
<t>
When reducing a variable-length argument data item,
the Requester MUST NOT include the data item's XDR roundup
padding in the chunk itself.
The chunk's total length MUST be the same as the encoded length of the data item.
</t>
</section>

</section>

<section
 anchor="section:b8492157-7734-43e8-9a90-8ee32c674a12"
 title="Write Chunks">
<t>
While constructing an RPC Call message,
a Requester prepares memory regions
in which to receive DDP-eligible result data items.
A "Write chunk" represents an XDR data item that is to be pushed
from a Responder to a Requester.
It is made up of an array of zero or more plain segments.
</t>
<t>
Write chunks are provisioned by a Requester long
before the Responder has prepared the reply Payload stream.
A Requester often does not know the actual length
of the result data items to be returned,
since the result does not yet exist.
Thus, it MUST register Write chunks long enough
to accommodate the maximum possible size of each returned data item.
</t>
<t>
In addition, the XDR position of DDP-eligible data items
in the reply's Payload stream is not predictable
when a Requester constructs an RPC Call message.
Therefore, RDMA segments in a Write chunk do not have a Position field.
</t>
<t>
For each Write chunk provided by a Requester,
the Responder pushes one data item to the Requester,
filling the chunk contiguously and in segment array order
until that data item has been completely written
to the Requester.
The Responder MUST copy the segment count
and all segments from the Requester-provided Write chunk
into the RPC Reply message's Transport header.
As it does so, the Responder updates each segment length field
to reflect the actual amount of data that is being returned in that segment.
The Responder then sends the RPC Reply message via an RDMA Send operation.
</t>
<t>An "empty Write chunk" is a Write chunk with a zero segment count.
By definition, the length of an empty Write chunk is zero.
An "unused Write chunk" has a non-zero segment count,
but all of its segments are empty segments.
</t>

<section
 anchor="section:2e73c718-e091-4d41-a88a-0b28c84fbe5c"
 title="Decoding Write Chunks">
<t>
After receiving the RPC Reply message,
the Requester reconstructs the transferred data
by concatenating the contents of each segment in array order
into the RPC Reply message's XDR stream at the known XDR position
of the associated DDP-eligible result data item.
</t>
</section>

<section
 anchor="section:00f1edd4-2e2f-4b54-bc21-9d6dfa39556b"
 title="Write Chunk Roundup">
<t>
When provisioning a Write chunk for a variable-length result data item,
the Requester MUST NOT include additional space for XDR roundup padding.
A Responder MUST NOT write XDR roundup padding into a Write chunk,
even if the result is shorter than the available space in the chunk.
Therefore, when returning a single variable-length result data item,
a returned Write chunk's total length MUST be
the same as the encoded length of the result data item.
</t>
</section>

</section>

</section>

<section
 anchor="section:740e5b29-8c88-40ab-9506-69635d9a8167"
 title="Message Format">
<t>
Because a Responder is required to have previously posted
recieve resources, RPC/RDMA version 2 provides several
ways of conveying an RPC/RDMA message.
A sender chooses the specific format for a message
among several factors:
<list style="symbols">
<t>
The existence of DDP-eligible data items in the RPC message
</t>
<t>
The size of the RPC message to be transported
</t>
<t>
The direction of the RPC message (i.e., Call or Reply)
</t>
<t>
The hardware resources that are available
</t>
<t>
The arrangement of the sink memory buffers
</t>
</list>
The following subsections describe in detail how each format 
is constructued.
</t>

<section
 anchor="section:13a871a4-554f-4e3c-89ba-c5a75929f01E"
 title="Simple Format">
<t>
Since all RPC messages conveyed via RPC-over-RDMA version 2
require at least one RDMA Send operation,
the most efficient way to send an RPC message
that is smaller than the inline threshold
is to append the Payload stream directly to the Transport stream.
An RPC-over-RDMA header with a small RPC Call or Reply
message immediately following is transferred using a single
RDMA Send operation.
When no chunks are present,
Calls and Replies are constructed the same way, and
no other operations are needed.
</t>

<section
 anchor="section:41c2df3b-e54d-4ccf-9eac-860f44dec2d2"
 title="Simple Format with Chunks">
<t>
If DDP-eligible data items are present in a Payload stream,
a sender MAY reduce some or all of these items by removing them
from the Payload stream.
The sender then uses a separate mechanism to transfer the reduced data items.
The Transport stream with the reduced Payload stream immediately
following is then transferred using a single RDMA Send operation.
</t>
<t>
When chunks are present, 
Calls are handled differently than Replies.
<list style="hanging">
<t hangText="Simple Call"><vspace blankLines="0"/>
After receiving the Transport and Payload streams
of an RPC Call message accompanied by Read chunks,
the Responder uses RDMA Read operations to move reduced data items in Read chunks.
RPC/RDMA Replies never convey Read chunks.
</t>
<t hangText="Simple Reply"><vspace blankLines="0"/>
Before sending the Transport and Payload streams of an RPC Reply message
containing Write chunks, the Responder uses RDMA Write operations
to move reduced data items in Write and Reply chunks.
RPC/RDMA Calls can carry provisioned but unused Write chunks
for the Responder to use for the sending matching Reply.
</t>
</list>
</t>
</section>

<section
 anchor="section:a690cf12-0e31-4df2-a32a-8cb8e2a3b2c8"
 title="Simple Format Examples">
<t>
<figure
 align="left"
 title="A Simple Call without chunks and
        a Simple Reply without chunks">
<artwork xml:space="preserve" align="left">
        Requester                             Responder
            |        RDMA Send (RDMA_MSG)         |
       Call |   ------------------------------&gt;   |
            |                                     |
            |                                     | Processing
            |                                     |
            |        RDMA Send (RDMA_MSG)         |
            |   &lt;------------------------------   | Reply
</artwork>
</figure>
<figure
 align="left"
 title="A Simple Call with a Read chunk and
        a Simple Reply without chunks">
<artwork xml:space="preserve" align="left">
        Requester                             Responder
            |        RDMA Send (RDMA_MSG)         |
       Call |   ------------------------------&gt;   |
            |        RDMA Read                    |
            |   &lt;------------------------------   |
            |        RDMA Response (arg data)     |
            |   ------------------------------&gt;   |
            |                                     |
            |                                     | Processing
            |                                     |
            |        RDMA Send (RDMA_MSG)         |
            |   &lt;------------------------------   | Reply
</artwork>
</figure>
<figure
 align="left"
 title="A Simple Call without chunks and
        a Simple Reply with a Write chunk">
<artwork xml:space="preserve" align="left">
        Requester                             Responder
            |        RDMA Send (RDMA_MSG)         |
       Call |   ------------------------------&gt;   |
            |                                     |
            |                                     | Processing
            |                                     |
            |        RDMA Write (result data)     |
            |   &lt;------------------------------   |
            |        RDMA Send (RDMA_MSG)         |
            |   &lt;------------------------------   | Reply
</artwork>
</figure>
</t>
</section>

</section>

<section
 anchor="section:bda7bc14-2a3d-4224-873c-855912218987"
 title="Continued Format">
<t>
A sender can choose to split a message payload over multiple
RPC-over-RDMA messages.
The Payload stream of each RPC-over-RDMA message contains
a part of the RPC message.
The receiver reconstitutes the RPC message by concatenating
the Payload streams of the sequence
of RPC-over-RDMA messages together.
A Sender MAY split an RPC message payload on any convenient
boundary.
</t>

<section
 anchor="section:3b102dbf-81b2-4e0f-bb4b-d238db83ab44"
 title="Continued Format with Chunks">
<t>
When no chunks are present,
Calls and Replies are constructed the same way.
However, if DDP-eligible data items are present in a Payload stream,
a sender MAY reduce some or all of these items by removing them
from the Payload stream.
The sender then uses a separate mechanism to transfer the reduced data items.
The Transport stream with the reduced Payload stream immediately
following is then transferred using a single RDMA Send operation.
</t>
<t>
As with Simple Format messages, when chunks are present, 
Calls are handled differently than Replies.
<list style="hanging">
<t hangText="Continued Call"><vspace blankLines="0"/>
After receiving the Transport and Payload streams
of an RPC Call message accompanied by Read chunks,
the Responder uses RDMA Read operations to move reduced data items in Read chunks.
RPC/RDMA Replies never convey Read chunks.
</t>
<t hangText="Continued Reply"><vspace blankLines="0"/>
Before sending the Transport and Payload streams of an RPC Reply message
containing Write chunks, the Responder uses RDMA Write operations
to move reduced data items in Write and Reply chunks.
RPC/RDMA Calls can carry provisioned but unused Write chunks
for the Responder to use for the sending matching Reply.
</t>
</list>
</t>
</section>

<section
 anchor="section:49c71620-d55a-4867-801a-10928118befe"
 title="Continued Format Examples">
<t>
<figure
 align="left"
 title="A Continued Call without chunks and
        a Continued Reply without chunks">
<artwork xml:space="preserve" align="left">
        Requester                             Responder
            |        RDMA Send (RDMA_MSG)         |
       Call |   ------------------------------&gt;   |
            |        RDMA Send (RDMA_MSG)         |
            |   ------------------------------&gt;   |
            |        RDMA Send (RDMA_MSG)         |
            |   ------------------------------&gt;   |
            |                                     |
            |                                     |
            |                                     | Processing
            |                                     |
            |        RDMA Send (RDMA_MSG)         |
            |   &lt;------------------------------   | Reply
            |        RDMA Send (RDMA_MSG)         |
            |   &lt;------------------------------   |
            |        RDMA Send (RDMA_MSG)         |
            |   &lt;------------------------------   |
</artwork>
</figure>
<figure
 align="left"
 title="A Continued Call with a Read chunk and
        a Simple Reply without chunks">
<artwork xml:space="preserve" align="left">
        Requester                             Responder
            |        RDMA Send (RDMA_MSG)         |
       Call |   ------------------------------&gt;   |
            |        RDMA Send (RDMA_MSG)         |
            |   ------------------------------&gt;   |
            |        RDMA Send (RDMA_MSG)         |
            |   ------------------------------&gt;   |
            |        RDMA Read                    |
            |   &lt;------------------------------   |
            |        RDMA Response (arg data)     |
            |   ------------------------------&gt;   |
            |                                     |
            |                                     | Processing
            |                                     |
            |        RDMA Send (RDMA_MSG)         |
            |   &lt;------------------------------   | Reply
</artwork>
</figure>
<figure
 align="left"
 title="A Simple Call without chunks and
        a Continued Reply with a Write chunk">
<artwork xml:space="preserve" align="left">
        Requester                             Responder
            |        RDMA Send (RDMA_MSG)         |
       Call |   ------------------------------&gt;   |
            |                                     |
            |                                     | Processing
            |                                     |
            |        RDMA Write (result data)     |
            |   &lt;------------------------------   |
            |        RDMA Send (RDMA_MSG)         |
            |   &lt;------------------------------   | Reply
            |        RDMA Send (RDMA_MSG)         |
            |   &lt;------------------------------   |
            |        RDMA Send (RDMA_MSG)         |
            |   &lt;------------------------------   |
</artwork>
</figure>
</t>
</section>

</section>

<section
 anchor="section:d9731520-bdde-4a1b-9f54-9901d5c57648"
 title="Special Format">
<t>
If there are no DDP-eligible data items in the Payload stream,
or the Payload stream is still too large after it has been reduced,
the sender uses either Message Continuation,
or it can use RDMA Read or Write operations to convey the
entire RPC message.
The latter mechanism is referred to as a "Special Format" message.
</t>
<t>
To transmit a Special Format Message,
the sender conveys only the Transport stream
with an RDMA Send operation.
The Payload stream is not included in the Send buffer in this instance.
Instead, the Requester provides chunks that the Responder
uses to move the Payload stream.
Like other formats, because chunks are present,
Calls are handled differently than Replies.
<list style="hanging">
<t hangText="Special Format Call"><vspace blankLines="0"/>
The Requester provides a Read chunk
that contains the RPC Call message's Payload stream.
Every RDMA read segment in this chunk MUST contain zero in its Position field.
This type of chunk is known as a "Position Zero Read chunk".
</t>
<t hangText="Special Format Reply"><vspace blankLines="0"/>
The Requester provisions a single Write chunk in advance,
known as the "Reply chunk",
that will contain the RPC Reply message's Payload stream.
The Requester sizes the Reply chunk to accommodate the maximum expected
reply size for that upper-layer operation.
</t>
</list>
Though the purpose of a Special Format Message is to handle large RPC messages,
Requesters MAY use a Special Format Message at any time to convey an RPC Call message.
</t>
<t>
A Responder chooses which Format to use based on the chunks provided
by the Requester.
If Write chunks were provided and the Responder has a DDP-eligible result,
it first reduces the reply Payload stream.
If a Reply chunk was provided and the reduced Payload stream
is larger than the reply inline threshold,
the Responder MUST use the Requester-provided Reply chunk for the reply.
</t>
<t>
XDR data items may appear in these special chunks
without regard to their DDP-eligibility.
As these chunks contain a Payload stream,
such chunks MUST include appropriate XDR roundup padding
to maintain proper XDR alignment of their contents.
</t>

<section
 anchor="section:199de5c1-e307-4664-abeb-dd687b4329c3"
 title="Special Format Examples">
<t>
<figure
 align="left"
 title="A Special Call and a Simple Reply without chunks">
<artwork xml:space="preserve" align="left">
        Requester                             Responder
            |        RDMA Send (RDMA_NOMSG)       |
       Call |   ------------------------------&gt;   |
            |        RDMA Read                    |
            |   &lt;------------------------------   |
            |        RDMA Response (RPC call)     |
            |   ------------------------------&gt;   |
            |                                     |
            |                                     | Processing
            |                                     |
            |        RDMA Send (RDMA_MSG)         |
            |   &lt;------------------------------   | Reply
</artwork>
</figure>
</t>
<t>
<figure
 align="left"
 title="A Simple Call without chunks and a Special Reply">
<artwork xml:space="preserve" align="left">
        Requester                             Responder
            |        RDMA Send (RDMA_MSG)         |
       Call |   ------------------------------&gt;   |
            |                                     |
            |                                     | Processing
            |                                     |
            |        RDMA Write (RPC reply)       |
            |   &lt;------------------------------   |
            |        RDMA Send (RDMA_NOMSG)       |
            |   &lt;------------------------------   | Reply
</artwork>
</figure>
</t>
</section>

</section>

</section>

</section>

<section
 anchor="section:86248e99-ca60-478a-8aff-3fb387410077"
 title="Transport Properties">
<t>
RPC-over-RDMA version 2 provides a mechanism
for connection endpoints
to communicate information about implementation properties,
enabling compatible endpoints to optimize data transfer.
Initially only a small set of transport properties are defined
and a single operation is provided to exchange transport properties
(see
<xref target="section:07e8c178-62df-46a7-a57e-dcf107821d93"/>).
</t>
<t>
Both the set of transport properties and the operations used to
communicate may be extended.
Within RPC-over-RDMA version 2, all such extensions are OPTIONAL.
For information about existing transport properties, see Sections
<xref
 target="section:d5ac12f6-6735-48f3-b4ba-b44a19ff9298"
 pageno="false"
 format="counter"/>
through
<xref
 target="section:943010bd-c342-46b7-9fcd-df746437dd6f"
 pageno="false"
 format="counter"/>.
For discussion of extensions to the set of transport properties, see
<xref target="section:a355adad-f03b-41a6-94a8-4128b10301bb"/>.
</t>

<section
 anchor="section:d5ac12f6-6735-48f3-b4ba-b44a19ff9298"
 title="Transport Properties Model">
<t>
A basic set of receiver and sender properties is specified in this document.
An extensible approach is used, allowing new properties to be defined
in future Standards Track documents.
</t>
<t>
Such properties are specified using:
<list style="symbols">
<t>
A code point identifying the particular transport property being specified.
</t>
<t>
A nominally opaque array which contains within it the XDR encoding
of the specific property indicated by the associated code point.
</t>
</list>
</t>
<t>
The following XDR types are used by operations that deal with
transport properties:
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

typedef rpcrdma2_propid uint32;

struct rpcrdma2_propval {
        rpcrdma2_propid rdma_which;
        opaque          rdma_data&lt;&gt;;
};

typedef rpcrdma2_propval rpcrdma2_propset&lt;&gt;;

typedef uint32 rpcrdma2_propsubset&lt;&gt;;

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
<t>
An rpcrdma2_propid specifies a particular transport property.
In order to facilitate XDR extension of the set of properties
by concatenating XDR definition files,
specific properties are defined as const values
rather than as elements in an enum.
</t>
<t>
An rpcrdma2_propval specifies a value of a particular transport
property with the particular property identified by rdma_which,
while the associated value of that property is contained within rdma_data.
</t>
<t>
An rdma_data field which is of zero length is interpreted as
indicating the default value or the property indicated by rdma_which.
</t>
<t>
While rdma_data is defined as opaque within the XDR,
the contents are interpreted (except when of length zero)
using the XDR typedef associated with the property specified by rdma_which.
As a result, when rpcrdma2_propval does not conform to that typedef,
the receiver is REQUIRED to return the error RDMA2_ERR_BAD_XDR
using the header type RDMA2_ERROR as described in
<xref target="section:b1d23e5c-31df-483f-adb7-25430b5de38d"/>.
For example, the receiver of a message
containing a valid rpcrdma2_propval returns this error
if the length of rdma_data is such that it extends beyond
the bounds of the message being transferred.
</t>
<t>
In cases in which the rpcrdma2_propid specified by rdma_which is
understood by the receiver, the receiver also MUST report the error
RDMA2_ERR_BAD_XDR if either of the following occur:
<list style="symbols">
<t>
The nominally opaque data within rdma_data is not valid when
interpreted using the property-associated typedef.
</t>
<t>
The length of rdma_data is insufficient to contain the data
represented by the property-associated typedef.
</t>
</list>
Note that no error is to be reported if rdma_which is unknown to the receiver.
In that case, that rpcrdma2_propval is not processed and processing continues
using the next rpcrdma2_propval, if any.
</t>
<t>
A rpcrdma2_propset specifies a set of transport properties.
No particular ordering of the rpcrdma2_propval items within it is imposed.
</t>
<t>
A rpcrdma2_propsubset identifies a subset of the properties in a
previously specified rpcrdma2_propset.
Each bit in the mask denotes a particular element in a previously
specified rpcrdma2_propset.
If a particular rpcrdma2_propval is at position N in the array,
then bit number N mod 32 in word N div 32 specifies whether
that particular rpcrdma2_propval is included in the defined subset.
Words beyond the last one specified are treated as containing zero.
</t>
</section>

<section
 anchor="section:943010bd-c342-46b7-9fcd-df746437dd6f"
 title="Current Transport Properties">
<t>
Although the set of transport properties may be extended,
a basic set of transport properties is defined in
<xref target="table:99d0e7cc-da81-4f16-9bd0-471f806bc0b6"/>.
</t>
<t>
In that table, the columns contain the following information:
<list style="symbols">
<t>
The column labeled "Property" identifies the transport property
described by the current row.
</t>
<t>
The column labeled "Code" specifies the rpcrdma2_propid value used
to identify this property.
</t>
<t>
The column labeled "XDR type" gives the XDR type of the data used
to communicate the value of this property.
This data type overlays the data portion
of the nominally opaque field rdma_data in a rpcrdma2_propval.
</t>
<t>
The column labeled "Default" gives the default value for the
property which is to be assumed by those who do not receive,
or are unable to interpret,
information about the actual value of the property.
</t>
<t>
The column labeled "Sec" indicates the section within this
document that explains the semantics and use of this transport
property.
</t>
</list>
</t>
<texttable
 align="left"
 style="full"
 anchor="table:99d0e7cc-da81-4f16-9bd0-471f806bc0b6">
<ttcol align="left">Property</ttcol>
<ttcol align="left">Code</ttcol>
<ttcol align="left">XDR type</ttcol>
<ttcol align="left">Default</ttcol>
<ttcol align="left">Sec</ttcol>
<c>Maximum Send Size</c>
<c>1</c>
<c>uint32</c>
<c>4096</c>
<c>
<xref target="section:0a985ff5-a5c1-477f-8932-517be34ccf65"
 pageno="false"
 format="counter"/>
</c>
<c>Receive Buffer Size</c>
<c>2</c>
<c>uint32</c>
<c>4096</c>
<c>
<xref target="section:5101b1f1-b1ad-4b6b-9fa4-d6fa324ffc0d"
 pageno="false"
 format="counter"/>
</c>
<c>Maximum RDMA Segment Size</c>
<c>3</c>
<c>uint32</c>
<c>1048576</c>
<c>
<xref target="section:14ed280d-521c-410c-a190-cf891be53900"
 pageno="false"
 format="counter"/>
</c>
<c>Maximum RDMA Segment Count</c>
<c>4</c>
<c>uint32</c>
<c>16</c>
<c>
<xref target="section:98fc720d-6263-4a52-ae89-d2469b982512"
 pageno="false"
 format="counter"/>
</c>
<c>Reverse Request Support</c>
<c>5</c>
<c>uint32</c>
<c>1</c>
<c>
<xref target="section:6ace2d7f-044b-491f-97ea-5760345a2e8f"
 pageno="false"
 format="counter"/>
</c>
<c>Host Auth Message</c>
<c>6</c>
<c>opaque&lt;&gt;</c>
<c>N/A</c>
<c>
<xref target="section:5f63e1b6-8d24-453b-b18b-b98ad66f3671"
 pageno="false"
 format="counter"/>
</c>
</texttable>

<section
 anchor="section:0a985ff5-a5c1-477f-8932-517be34ccf65"
 title="Maximum Send Size">
<t>
The Maximum Send Size specifies the maximum size, in octets,
of Send payloads.
The endpoint sending this value ensures that it will not
transmit a Send WR payload larger than this size,
allowing the endpoint receiving this value to size its
Receive buffers appropriately.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

const uint32 RDMA2_PROPID_SBSIZ = 1;
typedef uint32 rpcrdma2_prop_sbsiz;

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
</section>

<section
 anchor="section:5101b1f1-b1ad-4b6b-9fa4-d6fa324ffc0d"
 title="Receive Buffer Size">
<t>
The Receive Buffer Size specifies the minimum size, in octets,
of pre-posted receive buffers.
It is the responsibility of the endpoint sending this value
to ensure that its pre-posted receive buffers are at least the size specified,
allowing the endpoint receiving this value to send messages
that are of this size.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

const uint32 RDMA2_PROPID_RBSIZ = 2;
typedef uint32 rpcrdma2_prop_rbsiz;

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
<t>
A sender may use his knowledge of the receiver's buffer size to
determine when the message to be sent will fit in the preposted
receive buffers that the receiver has set up.
In particular,
<list style="symbols">
<t>
Requesters may use the value to determine when it is necessary to
provide a Position Zero Read chunk or Message Continuation
when sending a request.
</t>
<t>
Requesters may use the value to determine when it is necessary to
provide a Reply chunk when sending a request, based on the maximum
possible size of the reply.
</t>
<t>
Responders may use the value to determine when it is necessary,
given the actual size of the reply, to actually use a Reply chunk
provided by the requester.
</t>
</list>
</t>
</section>

<section
 anchor="section:14ed280d-521c-410c-a190-cf891be53900"
 title="Maximum RDMA Segment Size">
<t>
The Maximum RDMA Segment Size specifies the maximum size, in octets,
of an RDMA segment this endpoint is prepared to send or receive.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

const uint32 RDMA2_PROPID_RSSIZ = 3;
typedef uint32 rpcrdma2_prop_rssiz;

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
</section>

<section
 anchor="section:98fc720d-6263-4a52-ae89-d2469b982512"
 title="Maximum RDMA Segment Count">
<t>
The Maximum RDMA Segment Count specifies the maximum number of
RDMA segments that can appear in a requester's transport header.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

const uint32 RDMA2_PROPID_RCSIZ = 4;
typedef uint32 rpcrdma2_prop_rcsiz;

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
</section>

<section
 anchor="section:6ace2d7f-044b-491f-97ea-5760345a2e8f"
 title="Reverse Request Support">
<t>
The value of this property is used to indicate a client implementation's
readiness to accept and process messages that are part
of reverse direction RPC requests.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

const uint32 RDMA_RVREQSUP_NONE = 0;
const uint32 RDMA_RVREQSUP_INLINE = 1;
const uint32 RDMA_RVREQSUP_GENL = 2;

const uint32 RDMA2_PROPID_BRS = 5;
typedef uint32 rpcrdma2_prop_brs;

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
<t>
Multiple levels of support are distinguished:
<list style="symbols">
<t>
The value RDMA2_RVREQSUP_NONE indicates that receipt of reverse
direction requests and replies is not supported.
</t>
<t>
The value RDMA2_RVREQSUP_INLINE indicates that receipt of reverse
direction requests or replies is only supported using inline
messages and that use of explicit RDMA operations
for reverse direction messages is not supported.
</t>
<t>
The value RDMA2_RVREQSUP_GENL that receipt of reverse direction
requests or replies is supported in the same ways that forward
direction requests or replies typically are.
</t>
</list>
</t>
<t>
When information about this property is not provided,
the support level of servers can be inferred
from the reverse direction requests that they issue,
assuming that issuing a request implicitly indicates support
for receiving the corresponding reply.
On this basis, support for receiving inline replies
can be assumed when requests without
Read chunks, Write chunks, or Reply chunks are issued,
while requests with any of these elements allow the client to assume
that general support for reverse direction replies is present on the server.
</t>
</section>

<section
 anchor="section:5f63e1b6-8d24-453b-b18b-b98ad66f3671"
 title="Host Authentication Message">
<t>
The value of this transport property is used
as part of an exchange of host authentication material.
This property can accommodate authentication handshakes that
require multiple challenge-response interactions, and
potentially large amounts of material.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

const uint32 RDMA2_PROPID_HOSTAUTH = 6;
typedef opaque rpcrdma2_prop_hostauth&lt;&gt;;

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
<t>
When this property is not provided, the peer(s) remain unauthenticated.
Local security policy on each peer determines
whether the connection is permitted to continue.
</t>
</section>

</section>

</section>

<section
 anchor="section:eef6a22e-2633-44a2-a8f0-821fec8bf824"
 title="RPC-over-RDMA Version 2 Transport Messages">

<section
 anchor="section:417e749e-efec-455f-aae7-12535b9ee8dc"
 title="Overall Transport Message Structure">
<t>
Each transport message consists of multiple sections:
<list style="symbols">
<t>
A transport header prefix, as defined in
<xref target="section:2d1735f0-c465-43c6-9c18-3da6b7979862"/>.
Among other things, this structure indicates the header type.
</t>
<t>
The transport header proper, as defined by one of the sub-sections below.
See
<xref target="section:67b34950-5376-49fd-93d7-b4fdf80d1c9b"/>
for the mapping between header types and the corresponding header structure.
</t>
<t>
Potentially, all or part of an RPC message payload
being conveyed as an addendum to the transport header.
</t>
</list>
</t>
<t>
This organization differs from that presented in the definition of
RPC-over-RDMA version 1
<xref target="RFC8166"/>,
which presented the first and second of the items above as a single XDR item.
The new organization is more in keeping with RPC-over-RDMA version 2's
extensibility model in that new header types can be defined without
modifying the existing set of header types.
</t>
</section>

<section
 anchor="section:67b34950-5376-49fd-93d7-b4fdf80d1c9b"
 title="Transport Header Types">
<t>
The new header types within RPC-over-RDMA version 2
are set forth in
<xref target="table:b5c31bf9-d623-4957-97db-29fc1d416cb8"/>.
In that table, the columns contain the following information:
<list style="symbols">
<t>
The column labeled "Operation" specifies the particular operation.
</t>
<t>
The column labeled "Code" specifies the value of header type for
this operation.
</t>
<t>
The column labeled "XDR type" gives the XDR type of the data
structure used to describe the information in this new message type.
This data immediately follows the universal portion on the
transport header present in every RPC-over-RDMA transport header.
</t>
<t>
The column labeled "Msg" indicates whether this operation is
followed (or not) by an RPC message payload.
</t>
<t>
The column labeled "Sec" indicates the section (within this
document) that explains the semantics and use of this operation.
</t>
</list>
</t>
<texttable
 align="left"
 anchor="table:b5c31bf9-d623-4957-97db-29fc1d416cb8"
 style="full"
 suppress-title="false"
 title="">
<ttcol align="left">Operation</ttcol>
<ttcol align="left">Code</ttcol>
<ttcol align="left">XDR type</ttcol>
<ttcol align="left">Msg</ttcol>
<ttcol align="left">Sec</ttcol>
<c>Convey Appended RPC Message</c>
<c>0</c>
<c>rpcrdma2_msg</c>
<c>Yes</c>
<c>
<xref target="section:9af0d451-2ef3-454f-adb9-827664ccc39c"
 pageno="false"
 format="counter"/>
</c>
<c>Convey External RPC Message</c>
<c>1</c>
<c>rpcrdma2_nomsg</c>
<c>No</c>
<c>
<xref target="section:1c401555-4b7d-4e35-a9a3-8aa14228170e"
 pageno="false"
 format="counter"/>
</c>
<c>Report Transport Error</c>
<c>4</c>
<c>rpcrdma2_err</c>
<c>No</c>
<c>
<xref target="section:b1d23e5c-31df-483f-adb7-25430b5de38d"
 pageno="false"
 format="counter"/>
</c>
<c>Specify Properties at Connection</c>
<c>5</c>
<c>rpcrdma2_connprop</c>
<c>No</c>
<c>
<xref target="section:07e8c178-62df-46a7-a57e-dcf107821d93"
 pageno="false"
 format="counter"/>
</c>
</texttable>
<t>
Suppport for the operations in
<xref target="table:b5c31bf9-d623-4957-97db-29fc1d416cb8"/>
is REQUIRED.
Support for additional operations will be OPTIONAL.
RPC-over-RDMA version 2 implementations that receive an OPTIONAL operation
that is not supported MUST respond with an RDMA2_ERROR message
with an error code of RDMA2_ERR_INVAL_HTYPE.
</t>
</section>

<section
 anchor="section:2e577c75-4e43-4e13-8b17-75afa849f0b6"
 title="RPC-over-RDMA Version 2 Headers and Chunks">
<t>
Most RPC-over-RDMA version 2 data structures are derived
from corresponding structures in RPC-over-RDMA version 1.
As is typical for new versions of an existing protocol,
the XDR data structures have new names and there are a
few small changes in content.
In some cases,
there have been structural re-organizations to enabled
protocol extensibility.
</t>

<section
 anchor="section:e21d4f74-b536-47f2-9d07-c03a27a20de4"
 title="Common Transport Header Prefix">
<t>
The rpcrdma_common prefix describes the first part
of each RDMA-over-RPC transport header for version 2
and subsequent versions.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

struct rpcrdma_common {
             uint32         rdma_xid;
             uint32         rdma_vers;
             uint32         rdma_credit;
             uint32         rdma_htype;
};

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
<t>
RPC-over-RDMA version 2's use of these first four words
matches that of version 1 as required by
<xref target="RFC8166"/>.
However, there are important structural differences
in the way that these words are described
by the respective XDR descriptions:
<list style="symbols">
<t>
The header type is represented as a uint32 rather than as an enum
that would need to be modified to reflect additions to the set of
header types made by later extensions.
</t>
<t>
The header type field is part of an XDR structure devoted to
representing the transport header prefix,
rather than being part of a discriminated union,
that includes the body of each transport header type.
</t>
<t>
There is now a prefix structure
(see
<xref target="section:2d1735f0-c465-43c6-9c18-3da6b7979862"/>)
of which the rpcrdma_common structure is the initial segment.
This is a newly defined XDR object within the protocol description,
in contrast with RPC-over-RDMA version 1, which limits the common
portion of all header types to the four words in rpcrdma_common.
</t>
</list>
These changes are part of a larger structural change
in the XDR description of RPC-over-RDMA version 2
that enables a cleaner treatment of protocol extension.
The XDR appearing in
<xref target="section:bf53e759-d97f-487d-a5e2-9b8153db1803"/>
reflects these changes, which are discussed in further detail in
<xref target="section:d945b9f0-0666-4db7-9126-be57cf7b5f4f"/>.
</t>
</section>

<section
 anchor="section:2d1735f0-c465-43c6-9c18-3da6b7979862"
 title="RPC-over-RDMA Version 2 Transport Header Prefix">
<t>
The following prefix structure appears at the start of any
RPC-over-RDMA version 2 transport header.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

const RPCRDMA2_F_RESPONSE           0x00000001;
const RPCRDMA2_F_MORE               0x00000002;

struct rpcrdma2_hdr_prefix
        struct rpcrdma_common       rdma_start;
        uint32                      rdma_flags;
};

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
<t>
The rdma_flags is new to RPC-over-RDMA version 2.
Currently, the only flags defined within this word are the
RPCRDMA2_F_RESPONSE flag
and the
RPCRDMA2_F_MORE flag.
The other bits are reserved for future use as described in
<xref target="section:55a1749d-702c-48e4-b78c-0db392692c09"/>.
The sender MUST set these flags to zero.
</t>

<section
 anchor="section:4e23391c-b6a8-46b1-a399-c86f754b51bc"
 title="RPCRDMA2_F_RESPONSE Flag">
<t>
The RPCRDMA2_F_RESPONSE flag qualifies the value contained in the
transport header's rdma_start.rdma_xid field.
The RPCRDMA2_F_RESPONSE flag enables a receiver to reliably avoid
performing an XID lookup on incoming reverse direction Call messages.
</t>
<t>
In general, when a message carries an XID that was generated
by the message's receiver
(that is, the receiver is acting as a requester),
the message's sender sets the RPCRDMA2_F_RESPONSE flag.
Otherwise that flag is clear.
For example:
<list style="symbols">
<t>
When the rdma_start.rdma_htype field has the value RDMA2_MSG or
RDMA2_NOMSG, the value of the RPCRDMA2_F_RESPONSE flag MUST be the
same as the value of the associated RPC message's msg_type field.
</t>
<t>
When the header type is anything else
and
a whole or partial RPC message payload is present,
the value of the RPCRDMA2_F_RESPONSE flag MUST be the same
as the value of the associated RPC message's msg_type field.
</t>
<t>
When no RPC message payload is present,
a requester MUST set the value of RPCRDMA2_F_RESPONSE
to reflect how the receiver is to interpret the
rdma_start.rdma_xid field.
</t>
<t>
When the rdma_start.rdma_htype field has the value RDMA2_ERROR,
the RPCRDMA2_F_RESPONSE flag MUST be set.
</t>
</list>
</t>
</section>

<section
 anchor="section:fb299673-74f3-4f01-adb5-26a02ccd679f"
 title="RPCRDMA2_F_MORE Flag">
<t>
The RPCRDMA2_F_MORE flag signifies that
the RPC-over-RDMA message payload continues in the next message.
This is referred to as Message Continuation, or Send chaining.
</t>
<t>
When the RPCRDMA2_F_MORE flag is asserted,
the receiver is to concatenate the data payload of the next
received message to the end of the data payload of the
current received message.
The sender clears the RPCRDMA2_F_MORE flag in the final
message in the sequence.
</t>
<t>
All RPC-over-RDMA messages in such a sequence MUST have the same values
in the rdma_start.rdma_xid and rdma_start.rdma_htype fields.
If this constraint is not met,
the receiver MUST respond with an RDMA2_ERROR message
with the rdma_err field set to RDMA2_ERR_INVAL_FLAG.
</t>
<t>
If a peer receives an RPC-over-RDMA message
where the RPCRDMA2_F_MORE flag is set
and
the rdma_start.rdma_htype field does not contain
RDMA2_MSG or RDMA2_CONNPROP,
the receiver MUST respond with
an RDMA2_ERROR message with the rdma_err field set to RDMA2_ERR_INVAL_FLAG.
</t>
<t>
[ dnoveck: Both the above and your error in the existing third paragraph
raise issues since they could be sent by a responder.
Will need to fix RDMA2_ERROR so that this can be done when appropriate. ]
</t>
<t>
When the RPCRDMA2_F_MORE flag is set in an individual message,
that message's chunk lists MUST be empty.
Chunks for a chained message may be conveyed in the final
message in the sequence,
whose RPCRDMA2_F_MORE flag is clear.
</t>
<t>
There is no protocol-defined limit
on the number of concatenated messages in a sequence.
If the sender exhausts the receiver's credit grant before
the final message is sent,
the sender MUST wait for a further credit grant from
the receiver before continuing to send messages.
</t>
<t>
Credit exhaustion can occur at the receiver
in the middle of a sequence of continued messages.
To enable the sender to continue sending
the remaining messages in the sequence,
the receiver can grant more credits by
sending an RPC message payload
or
an out-of-band credit grant
(see
<xref target="section:f6348562-97f8-4413-96d7-bde3ff57b375"/>).
</t>
</section>

</section>

<section
 anchor="section:af116198-1815-4308-99ab-0197b2c5ea0b"
 title="Describing External Data Payloads">
<t>
The rpcrdma2_chunk_lists structure specifies how an RPC message
is conveyed using explicit RDMA operations.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

struct rpcrdma2_chunk_lists {
        uint32                      rdma_inv_handle;
        struct rpcrdma2_read_list   *rdma_reads;
        struct rpcrdma2_write_list  *rdma_writes;
        struct rpcrdma2_write_chunk *rdma_reply;
};

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
<t>
For the most part this structure parallels
its RPC-over-RDMA version 1 equivalent.
That is, the rdma_reads, rdma_writes, rdma_reply fields provide,
respectively,
descriptions of the chunks used to read a Special Format message or directly
placed data from the requester, to write directly placed response
data into the requester's memory, and to write a long reply into the
requester's memory.
</t>

<section
 anchor="section:ac98f12c-4662-48f5-a18d-48e652f0d4d7"
 title="Chunks and Chunk Lists">
<t>
The chunks and chunk list structures follow the same rules as in
Section 3.4 of
<xref target="RFC8166"/>,
with these exceptions:
<list style="symbols">
<t>
In RPC-over-RDMA version 1,
there were cases where XDR padding was allowed to appear in
a reduced XDR data item.
However, in RPC-over-RDMA version 2,
requesters and responders MUST NOT
include XDR padding in reduced Read and Write chunks, but
chunks that make up Position Zero Read chunks and Reply chunks
MUST include all XDR padding.
</t>
<t>
A responder MUST use Message Continuation
if the requester does not provide a Reply chunk and the
actual size of the reply is larger than the connection's
inline threshold.
A responder MAY use Message Continuation even if the
requester has provided adequate Reply resources.
This makes it unnecessary for
RPC-over-RDMA version 2 requesters to have perfect reply size estimation.
</t>
</list>
</t>
</section>

<section
 anchor="section:a957db67-a8fd-4886-b7a7-57382cfe3190"
 title="Remote Invalidation">
<t>
An important addition relative to the corresponding RPC-over-RDMA version 1
rdma_header structures is the rdma_inv_handle field.
This field supports remote invalidation
of requester memory registrations
via the RDMA Send With Invalidate operation.
</t>
<t>
To request Remote Invalidation, a requester sets the value of the
rdma_inv_handle field in an RPC Call's transport header to a non-zero
value that matches one of the rdma_handle fields in that header.  If
none of the rdma_handle values in the header conveying the Call may
be invalidated by the responder, the requester sets the RPC Call's
rdma_inv_handle field to the value zero.
</t>
<t>
If the responder chooses not to use remote invalidation for this
particular RPC Reply, or the RPC Call's rdma_inv_handle field
contains the value zero, the responder uses RDMA Send to transmit the
matching RPC reply.
</t>
<t>
If a requester has provided a non-zero value in the RPC Call's
rdma_inv_handle field and the responder chooses to use Remote
Invalidation for the matching RPC Reply, the responder uses RDMA Send
With Invalidate to transmit that RPC reply, and uses the value in the
corresponding Call's rdma_inv_handle field to construct the Send With
Invalidate Work Request.
</t>
</section>

</section>

</section>

<section
 anchor="section:8039c7b8-9068-401e-9cbd-5c1e67d403e7"
 title="Header Types Defined in RPC-over-RDMA version 2">
<t>
The header types defined and used in RPC-over-RDMA version 1
are all carried over into RPC-over-RDMA version 2,
although there may be limited changes
in the definition of existing header types.
</t>
<t>
In comparison with the header types of RPC-over-RDMA version 1,
the changes can be summarized as follows:
<list style="symbols">
<t>
To simplify interoperability with RPC-over-RDMA version 1,
only the RDMA2_ERROR header (defined in
<xref target="section:b1d23e5c-31df-483f-adb7-25430b5de38d"/>)
has an XDR definition that differs from that in RPC-over-RDMA version 1,
and its modifications are all compatible extensions.
</t>
<t>
RDMA2_MSG and RDMA2_NOMSG
(defined in Sections
<xref target="section:9af0d451-2ef3-454f-adb9-827664ccc39c"/>
and
<xref target="section:1c401555-4b7d-4e35-a9a3-8aa14228170e"/>)
have XDR definitions that match the corresponding
RPC-over-RDMA version 1 header types.
However, because of the changes to the header prefix,
the version 1 and version 2 header types
differ in on-the-wire format.
</t>
<t>
RDMA2_CONNPROP
(defined in
<xref target="section:07e8c178-62df-46a7-a57e-dcf107821d93"/>)
is a completely new header type devoted to enabling
connection peers to exchange information about their transport properties.
</t>
</list>
</t>

<section
 anchor="section:9af0d451-2ef3-454f-adb9-827664ccc39c"
 title="RDMA2_MSG: Convey RPC Message Inline">
<t>
RDMA2_MSG is used to convey an RPC message that immediately
follows the Transport Header in the Send buffer.
This is either an RPC request that has no Position Zero Read chunk
or an RPC reply that is not sent using a Reply chunk.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

const rpcrdma2_proc RDMA2_MSG = 0;

struct rpcrdma2_msg {
        struct rpcrdma2_chunk_lists  rdma_chunks;

        /* The rpc message starts here and continues
         * through the end of the transmission. */
        uint32                       rdma_rpc_first_word;
};

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
</section>

<section
 anchor="section:1c401555-4b7d-4e35-a9a3-8aa14228170e"
 title="RDMA2_NOMSG: Convey External RPC Message">
<t>
RDMA2_NOMSG can convey an entire RPC message payload using explicit RDMA operations.
When an RPC message payload is present, this message type is also known as a Special Format message.
In particular, it is a Special Format Call
when the Responder reads the RPC payload from a memory area specified by a Position Zero Read chunk;
and it is a Special Format Reply
when the Responder writes the RPC payload into a memory area specified by a Reply chunk.
In both of these cases, the rdma_xid field is set to the same value
as the xid of the RPC message payload.
</t>
<t>
If all the chunk lists are empty
(i.e., three 32-bit zeroes in the chunk list fields),
the message conveys a credit grant refresh.
The header prefix of this message contains a credit grant refresh
in the rdma_credit field.
In this case, the sender MUST set the rdma_xid field to zero.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

const rpcrdma2_proc RDMA2_NOMSG = 1;

struct rpcrdma2_nomsg {
        struct rpcrdma2_chunk_lists  rdma_chunks;
};

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
<t>
In RPC-over-RDMA version 2,
an alternative to using a Special Format message is to
use Message Continuation.
</t>
</section>

<section
 anchor="section:b1d23e5c-31df-483f-adb7-25430b5de38d"
 title="RDMA2_ERROR: Report Transport Error">
<t>
RDMA2_ERROR provides a way of reporting the occurrence of transport
errors on a previous transmission.
This header type MUST NOT be transmitted by a requester.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

const rpcrdma2_proc RDMA2_ERROR = 4;

struct rpcrdma2_err_vers {
        uint32 rdma_vers_low;
        uint32 rdma_vers_high;
};

struct rpcrdma2_err_write {
        uint32 rdma_chunk_index;
        uint32 rdma_length_needed;
};

union rpcrdma2_error switch (rpcrdma2_errcode rdma_err) {
        case RDMA2_ERR_VERS:
          rpcrdma2_err_vers rdma_vrange;
        case RDMA2_ERR_READ_CHUNKS:
          uint32 rdma_max_chunks;
        case RDMA2_ERR_WRITE_CHUNKS:
          uint32 rdma_max_chunks;
        case RDMA2_ERR_SEGMENTS:
          uint32 rdma_max_segments;
        case RDMA2_ERR_WRITE_RESOURCE:
          rpcrdma2_err_write rdma_writeres;
        case RDMA2_ERR_REPLY_RESOURCE:
          uint32 rdma_length_needed;
        default:
          void;
};

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
<t>
Error reporting is addressed in RPC-over-RDMA version 2
in a fashion similar to RPC-over-RDMA version 1.
Several new error codes, and error messages
never flow from requester to responder.
RPC-over-RDMA version 1 error reporting
is described in Section 5 of
<xref target="RFC8166"/>.
</t>
<t>
Unless otherwise specified, in all cases below,
the responder copies the values of the
rdma_start.rdma_xid
and
rdma_start.rdma_vers
fields from the incoming transport header that
generated the error to transport header of the error response.
The responder sets the rdma_start.rdma_htype field of the transport
header prefix to RDMA2_ERROR, and the rdma_start.rdma_credit field is
set to the credit grant value for this connection.
The receiver of this header type MUST ignore the value of the
rdma_start.rdma_credit field.
</t>
<t>
<list style="hanging">
<t hangText="RDMA2_ERR_VERS">
<vspace/>
This is the equivalent of ERR_VERS in RPC-over-RDMA version 1.
The error code value, semantics, and utilization are the same.
</t>
<t hangText="RDMA2_ERR_INVAL_HTYPE">
<vspace/>
If a responder recognizes the value in the rdma_start.rdma_vers field,
but it does not recognize the value in the rdma_start.rdma_htype field
or does not support that header type,
it MUST set the rdma_err field to RDMA2_ERR_INVAL_HTYPE.
</t>
<t hangText="RDMA2_ERR_INVAL_FLAG">
<vspace/>
If a receiver recognizes the value in the rdma_start.rdma_htype
field but does not recognize the combination of flags in the rdma_flags field,
it MUST set the rdma_err field to RDMA2_ERR_INVAL_HTYPE.
</t>
<t hangText="RDMA2_ERR_BAD_XDR">
<vspace/>
If a responder recognizes the values in the
rdma_start.rdma_vers
and
rdma_start.rdma_proc
fields,
but the incoming RPC-over-RDMA transport header cannot be parsed,
it MUST set the rdma_err field to RDMA2_ERR_BAD_XDR.
This includes cases in which a nominally opaque property value
field cannot be parsed
using the XDR typedef associated with the transport property definition.
The error code value of RDMA2_ERR_BAD_XDR is the same as
the error code value of ERR_CHUNK in RPC-over-RDMA version 1.
The responder MUST NOT process the request in any way
except to send an error message.
</t>
<t hangText="RDMA2_ERR_READ_CHUNKS">
<vspace/>
If a requester presents more DDP-eligible arguments than the responder
is prepared to Read,
the responder MUST set the rdma_err field to RDMA2_ERR_READ_CHUNKS,
and set the rdma_max_chunks field to the maximum number of
Read chunks the responder can receive and process.
<vspace/>
If the responder implementation cannot handle any Read chunks
for a request, it MUST set the rdma_max_chunks to zero in this response.
The requester SHOULD resend the request using a Position Zero Read chunk.
If this was a request using a Position Zero Read chunk,
the requester MUST terminate the transaction with an error.
</t>
<t hangText="RDMA2_ERR_WRITE_CHUNKS">
<vspace/>
If a requester has constructed an RPC Call message with
more DDP-eligible results than the server is prepared to Write,
the responder MUST set the rdma_err field to RDMA2_ERR_WRITE_CHUNKS,
and set the rdma_max_chunks field to the maximum number of
Write chunks the responder can process and return.
<vspace/>
If the responder implementation cannot handle any Write chunks for a
request, it MUST return a response of RDMA2_ERR_REPLY_RESOURCE (below).
The requester SHOULD resend the request with no Write chunks and
a Reply chunk of appropriate size.
</t>
<t hangText="RDMA2_ERR_SEGMENTS">
<vspace/>
If a requester has constructed an RPC Call message with a
chunk that contains more segments than the responder supports,
the responder MUST set the rdma_err field to RDMA2_ERR_SEGMENTS,
and set the rdma_max_segments field to the maximum number of
segments the responder can process.
</t>
<t hangText="RDMA2_ERR_WRITE_RESOURCE">
<vspace/>
If a requester has provided a Write chunk that is not large enough
to fully convey a DDP-eligible result,
the responder MUST set the rdma_err field to RDMA2_ERR_WRITE_RESOURCE.
<vspace/>
<vspace/>
The responder MUST set the rdma_chunk_index field to point to the
first Write chunk in the transport header that is too short, or to
zero to indicate that it was not possible to determine which chunk
is too small.
Indexing starts at one (1), which represents the first Write chunk.
The responder MUST set the rdma_length_needed to the number of bytes
needed in that chunk in order to convey the result data item.
<vspace/>
<vspace/>
Upon receipt of this error code,
a responder MAY choose to terminate the operation
(for instance, if the responder set the index and length fields to zero),
or it MAY send the request again using the same XID and more
reply resources.
</t>
<t hangText="RDMA2_ERR_REPLY_RESOURCE">
<vspace/>
If an RPC Reply's Payload stream does not fit inline
and the requester has not provided a large enough Reply chunk
to convey the stream,
the responder MUST set the rdma_err field to RDMA2_ERR_REPLY_RESOURCE.
The responder MUST set the rdma_length_needed to the number of
Reply chunk bytes needed to convey the reply.
<vspace/>
<vspace/>
Upon receipt of this error code,
a responder MAY choose to terminate the operation
(for instance, if the responder set the index and length fields to zero),
or it MAY send the request again using the same XID and larger
reply resources.
</t>
<t hangText="RDMA2_ERR_SYSTEM">
<vspace/>
If some problem occurs on a responder that does not fit
into the above categories,
the responder MAY report it to the sender by setting
the rdma_err field to RDMA2_ERR_SYSTEM.
<vspace/>
<vspace/>
This is a permanent error: a requester that receives this error MUST
terminate the RPC transaction associated with the XID value in the
rdma_start.rdma_xid field.
</t>
</list>
</t>
</section>

<section
 anchor="section:07e8c178-62df-46a7-a57e-dcf107821d93"
 title="RDMA2_CONNPROP: Advertise Transport Properties">
<t>
The RDMA2_CONNPROP message type allows an RPC-over-RDMA endpoint,
whether client or server, to indicate to its partner relevant
transport properties that the partner might need to be aware of.
</t>
<t>
The message definition for this operation is as follows:
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

struct rpcrdma2_connprop {
        rpcrdma2_propset rdma_props;
};

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
<t>
All relevant transport properties that the sender is aware of should
be included in rdma_props.
Since support of each of the properties is OPTIONAL,
the sender cannot assume that the receiver will necessarily take note
of these properties.
The sender should be prepared for cases in which the receiver
continues to assume that the default value for a particular property
is still in effect.
</t>
<t>
Generally, a participant will send a RDMA2_CONNPROP message as the
first message after a connection is established.
Given that fact, the sender should make sure that the message
can be received by peers who use the default Receive Buffer Size.
The connection's initial receive buffer size is typically 1KB,
but it depends on the initial connection state of the RPC-over-RDMA
version in use.
</t>
<t>
Properties not included in rdma_props are to be treated by the peer
endpoint as having the default value and are not allowed to change
subsequently.
The peer should not request changes in such properties.
</t>
<t>
Those receiving an RDMA2_CONNPROP may encounter properties that they
do not support or are unaware of.
In such cases, these properties are simply ignored
without any error response being generated.
</t>
</section>

</section>

<section
 anchor="section:19fc8c94-d83f-4c1b-8ce3-700918d129b5"
 title="Choosing a Reply Mechanism">
<t>
A requester provides any necessary registered memory resources for both
an RPC Call message
and
its matching RPC Reply message.
A requester forms each RPC Call itself,
thus it can compute the exact memory resources needed to send every Call.
However, the requester must allocate memory resources
to receive the corresponding Reply before the responder has formed it.
In some cases it is difficult for the requester to know in advance
precisely what resources will be needed to receive the Reply.
</t>
<t>
In RPC-over-RDMA version 2,
a requester MAY provide a Reply chunk at any time.
The responder MAY use the provided Reply chunk
or decide to use another means to convey the RPC Reply.
If the combination of the provided Write chunk list and Reply chunk
is not adequate to convey a Reply, the responder SHOULD use
Message Continuation (see
<xref target="section:fb299673-74f3-4f01-adb5-26a02ccd679f"/>
to send that Reply.
</t>
<t>
If even that is not possible, the responder sends
an RDMA2_ERROR message to the requester, as described in
<xref target="section:b1d23e5c-31df-483f-adb7-25430b5de38d"/>:
<list style="symbols">
<t>
The responder MUST send a RDMA2_ERR_WRITE_RESOURCE error
if the Write chunk list cannot accommodate the ULP's DDP-eligible data payload.
</t>
<t>
The responder MUST send a RDMA2_ERR_REPLY_RESOURCE error
if the Reply chunk cannot accommodate the non DDP-eligible parts of the Reply.
</t>
</list>
When receiving such errors, the requester SHOULD retry
the ULP call using larger reply resources.
In cases where retrying the ULP request is not possible,
the requester terminates the RPC request and presents
an error to the RPC consumer.
</t>
</section>


</section>

<section
 anchor="section:bf53e759-d97f-487d-a5e2-9b8153db1803"
 title="XDR Protocol Definition">
<t>
This section contains a description of the core features of
the RPC-over-RDMA version 2 protocol expressed in the XDR language
<xref target="RFC4506"/>.
</t>
<t>
Because of the need to provide for protocol extensibility
without modifying an existing XDR definition,
this description has some important structural differences
from the corresponding XDR description for RPC-over-RDMA version 1,
which appears in
<xref target="RFC8166"/>.
</t>
<t>
This description is divided into three parts:
<list style="symbols">
<t>
A code component license which appears in
<xref target="section:aaab9699-eae3-46ca-a1d5-a8776a5ecb7d"/>.
</t>
<t>
An XDR description of the structures that are generally available
for use by transport header types including both those defined in
this document and those that may be defined as extensions.
This includes definitions of the chunk-related structures
derived from RPC-over-RDMA version 1,
the transport property model introduced in this document,
and a definition of the transport header prefixes that precede the
various transport header types.
This appears in
<xref target="section:b25ffcfc-511f-4383-8025-4a68cfcb4f49"/>.
</t>
<t>
An XDR description of the transport header types defined in this document,
including those derived from RPC-over-RDMA version 1
and those introduced in RPC-over-RDMA version 2.
This appears in
<xref target="section:84e950a5-c842-4d19-b56d-0458c3e219b2"/>.
</t>
</list>
</t>
<t>
This description is provided in a way that makes it simple
to extract into ready-to-compile form.
To enable the combination of this description with the descriptions
of subsequent extensions to RPC-over-RDMA version 2,
the extracted description can be combined with similar descriptions
published later, or those descriptions can be compiled separately.
Refer to
<xref target="section:a288b3e6-5e73-412d-91e8-f87c031cb05b"/>
for details.
</t>

<section
 anchor="section:aaab9699-eae3-46ca-a1d5-a8776a5ecb7d"
 title="Code Component License">
<t>
Code components extracted from this document must include the
following license text.
When the extracted XDR code is combined with other complementary
XDR code which itself has an identical license, only a single
copy of the license text need be preserved.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

/// /*
///  * Copyright (c) 2010-2018 IETF Trust and the persons
///  * identified as authors of the code.  All rights reserved.
///  *
///  * The authors of the code are:
///  * B. Callaghan, T. Talpey, C. Lever, and D. Noveck.
///  *
///  * Redistribution and use in source and binary forms, with
///  * or without modification, are permitted provided that the
///  * following conditions are met:
///  *
///  * - Redistributions of source code must retain the above
///  *   copyright notice, this list of conditions and the
///  *   following disclaimer.
///  *
///  * - Redistributions in binary form must reproduce the above
///  *   copyright notice, this list of conditions and the
///  *   following disclaimer in the documentation and/or other
///  *   materials provided with the distribution.
///  *
///  * - Neither the name of Internet Society, IETF or IETF
///  *   Trust, nor the names of specific contributors, may be
///  *   used to endorse or promote products derived from this
///  *   software without specific prior written permission.
///  *
///  *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS
///  *   AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED
///  *   WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
///  *   IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
///  *   FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO
///  *   EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
///  *   LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
///  *   EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
///  *   NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
///  *   SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
///  *   INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
///  *   LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
///  *   OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
///  *   IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
///  *   ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
///  */
///

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
</section>

<section
 anchor="section:a288b3e6-5e73-412d-91e8-f87c031cb05b"
 title="Extraction and Use of XDR Definitions">
<t>
The reader can apply the following sed script to this
document to produce a machine-readable XDR description of
the RPC-over-RDMA version 2 protocol without any OPTIONAL
extensions.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

sed -n -e 's:^ */// ::p' -e 's:^ *///$::p'

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
<t>
That is, if this document is in a file called
"spec.txt" then the reader can do the following to extract
an XDR description file and store it in the file rpcrdma-v2.x.
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

sed -n -e 's:^ */// ::p' -e 's:^ *///$::p' \
     &lt; spec.txt &gt; rpcrdma-v2.x

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
<t>
Although this file is a usable description of the base protocol,
when extensions are to supported, it may be desirable to divide into
multiple files.
The following script can be used for that purpose:
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;

#!/usr/local/bin/perl
open(IN,"rpcrdma-v2.x");
open(OUT,"&gt;temp.x");
while(&lt;IN&gt;)
{
  if (m/FILE ENDS: (.*)$/)
    {
      close(OUT);
      rename("temp.x", $1);
      open(OUT,"&gt;temp.x");
    }
    else
    {
      print OUT $_;
    }
}
close(IN);
close(OUT);

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
<t>
Running the above script will result in two files:
<list style="symbols">
<t>
The file common.x, containing the license plus the common XDR
definitions which need to be made available to both the base
operations and any subsequent extensions.
</t>
<t>
The file baseops.x containing the XDR definitions for the base
operations, defined in this document.
</t>
</list>
</t>
<t>
Optional extensions to RPC-over-RDMA version 2,
published as Standards Track documents,
will have similar means of providing XDR that describes
those extensions.
Once XDR for all desired extensions is also extracted,
it can be appended to the XDR description file extracted
from this document to produce a consolidated XDR description
file reflecting all extensions selected for an RPC-over-RDMA
implementation.
</t>
<t>
Alternatively, the XDR descriptions can be compiled separately.
In this case the combination of common.x and baseops.x serves to
define the base transport, while using as XDR descriptions for
extensions, the XDR from the document defining that extension,
together with the file common.x, obtained from this document.
</t>
</section>

<section
 anchor="section:b25ffcfc-511f-4383-8025-4a68cfcb4f49"
 title="XDR Definition for RPC-over-RDMA Version 2 Core Structures">
<t>
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;
/// /*******************************************************************
///  *    Transport Header Prefixes
///  ******************************************************************/
///
/// struct rpcrdma_common {
///         uint32         rdma_xid;
///         uint32         rdma_vers;
///         uint32         rdma_credit;
///         uint32         rdma_htype;
/// };
///
/// const RPCRDMA2_F_RESPONSE           0x00000001;
/// const RPCRDMA2_F_MORE               0x00000002;
///
/// struct rpcrdma2_hdr_prefix
///         struct rpcrdma_common       rdma_start;
///         uint32                      rdma_flags;
/// };
///
/// /*******************************************************************
///  *    Chunks and Chunk Lists
///  ******************************************************************/
///
/// struct rpcrdma2_segment {
///         uint32 rdma_handle;
///         uint32 rdma_length;
///         uint64 rdma_offset;
/// };
///
/// struct rpcrdma2_read_segment {
///         uint32                  rdma_position;
///         struct rpcrdma2_segment rdma_target;
/// };
///
/// struct rpcrdma2_read_list {
///         struct rpcrdma2_read_segment rdma_entry;
///         struct rpcrdma2_read_list    *rdma_next;
/// };
///
/// struct rpcrdma2_write_chunk {
///         struct rpcrdma2_segment rdma_target&lt;&gt;;
/// };
///
/// struct rpcrdma2_write_list {
///         struct rpcrdma2_write_chunk rdma_entry;
///         struct rpcrdma2_write_list  *rdma_next;
/// };
///
/// struct rpcrdma2_chunk_lists {
///         uint32                      rdma_inv_handle;
///         struct rpcrdma2_read_list   *rdma_reads;
///         struct rpcrdma2_write_list  *rdma_writes;
///         struct rpcrdma2_write_chunk *rdma_reply;
/// };
///
/// /*******************************************************************
///  *    Transport Properties
///  ******************************************************************/
///
/// /*
///  * Types for transport properties model
///  */
/// typedef rpcrdma2_propid uint32;
///
/// struct rpcrdma2_propval {
///         rpcrdma2_propid rdma_which;
///         opaque          rdma_data&lt;&gt;;
/// };
///
/// typedef rpcrdma2_propval rpcrdma2_propset&lt;&gt;;
/// typedef uint32 rpcrdma2_propsubset&lt;&gt;;
///
/// /*
///  * Transport propid values for basic properties
///  */
/// const uint32 RDMA2_PROPID_SBSIZ = 1;
/// const uint32 RDMA2_PROPID_RBSIZ = 2;
/// const uint32 RDMA2_PROPID_RSSIZ = 3;
/// const uint32 RDMA2_PROPID_RCSIZ = 4;
/// const uint32 RDMA2_PROPID_BRS = 5;
/// const uint32 RDMA2_PROPID_HOSTAUTH = 6;
///
/// /*
///  * Types specific to particular properties
///  */
/// typedef uint32 rpcrdma2_prop_sbsiz;
/// typedef uint32 rpcrdma2_prop_rbsiz;
/// typedef uint32 rpcrdma2_prop_rssiz;
/// typedef uint32 rpcrdma2_prop_rcsiz;
/// typedef uint32 rpcrdma2_prop_brs;
/// typedef opaque rpcrdma2_prop_hostauth&lt;&gt;;
///
/// const uint32 RDMA_RVREQSUP_NONE = 0;
/// const uint32 RDMA_RVREQSUP_INLINE = 1;
/// const uint32 RDMA_RVREQSUP_GENL = 2;
///
/// /* FILE ENDS: common.x; */

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
</section>

<section
 anchor="section:84e950a5-c842-4d19-b56d-0458c3e219b2"
 title="XDR Definition for RPC-over-RDMA Version 2 Base Header Types">
<t>
<figure align="left">
<artwork xml:space="preserve" align="left">
&lt;CODE BEGINS&gt;
/// /*******************************************************************
///  *    Descriptions of RPC-over-RDMA Header Types
///  ******************************************************************/
///
/// /*
///  * Header Type Codes.
///  */
/// const rpcrdma2_proc RDMA2_MSG = 0;
/// const rpcrdma2_proc RDMA2_NOMSG = 1;
/// const rpcrdma2_proc RDMA2_ERROR = 4;
/// const rpcrdma2_proc RDMA2_CONNPROP = 5;
///
/// /*
///  * Header Types to Convey RPC Messages.
///  */
/// struct rpcrdma2_msg {
///         struct rpcrdma2_chunk_lists  rdma_chunks;
///
///         /* The rpc message starts here and continues
///          * through the end of the transmission. */
///         uint32                       rdma_rpc_first_word;
/// };
///
/// struct rpcrdma2_nomsg {
///         struct rpcrdma2_chunk_lists  rdma_chunks;
/// };
///
/// /*
///  * Header Type to Report Errors.
///  */
/// const uint32 RDMA2_ERR_VERS = 1;
/// const uint32 RDMA2_ERR_BAD_XDR = 2;
/// const uint32 RDMA2_ERR_INVAL_HTYPE = 3;
/// const uint32 RDMA2_ERR_INVAL_FLAG = 4;
/// const uint32 RDMA2_ERR_READ_CHUNKS = 5;
/// const uint32 RDMA2_ERR_WRITE_CHUNKS = 6;
/// const uint32 RDMA2_ERR_SEGMENTS = 7;
/// const uint32 RDMA2_ERR_WRITE_RESOURCE = 8;
/// const uint32 RDMA2_ERR_REPLY_RESOURCE = 9;
/// const uint32 RDMA2_ERR_SYSTEM = 10;
///
/// struct rpcrdma2_err_vers {
///         uint32 rdma_vers_low;
///         uint32 rdma_vers_high;
/// };
///
/// struct rpcrdma2_err_write {
///         uint32 rdma_chunk_index;
///         uint32 rdma_length_needed;
/// };
///
/// union rpcrdma2_error switch (rpcrdma2_errcode rdma_err) {
///         case RDMA2_ERR_VERS:
///           rpcrdma2_err_vers rdma_vrange;
///         case RDMA2_ERR_READ_CHUNKS:
///           uint32 rdma_max_chunks;
///         case RDMA2_ERR_WRITE_CHUNKS:
///           uint32 rdma_max_chunks;
///         case RDMA2_ERR_SEGMENTS:
///           uint32 rdma_max_segments;
///         case RDMA2_ERR_WRITE_RESOURCE:
///           rpcrdma2_err_write rdma_writeres;
///         case RDMA2_ERR_REPLY_RESOURCE:
///           uint32 rdma_length_needed;
///         default:
///           void;
/// };
///
/// /*
///  * Header Type to Exchange Transport Properties.
///  */
/// struct rpcrdma2_connprop {
///         rpcrdma2_propset rdma_props;
/// };
///
/// /* FILE ENDS: baseops.x; */

&lt;CODE ENDS&gt;
</artwork>
</figure>
</t>
</section>

<section
 anchor="section:5541f0da-efbb-4431-af9c-6f82aa773963"
 title="Use of the XDR Description Files">
<t>
The three files common.x and baseops.x,
when combined with the XDR descriptions for extension defined later,
produce a human-readable and compilable description
of the RPC-over-RDMA version 2 protocol with the included extensions.
</t>
<t>
Although this XDR description can be useful in generating code
to encode and decode the transport and payload streams,
there are elements of the structure of RPC-over-RDMA version 2
which are not expressible within the XDR language as currently defined.
This requires implementations that use the output of the XDR processor
to provide additional code to bridge the gaps.
<list style="symbols">
<t>
The values of transport properties are represented
within XDR as opaque values.
However, the actual structures of each
of the properties are represented by XDR typedefs,
with the selection of the appropriate typedef described by text in
this document.
The determination of the appropriate typedef is not specified by XDR,
which does not possess the facilities necessary for that determination
to be specified in an extensible way.
<vspace blankLines="1"/>
This is similar to the way in which NFSv4 attributes are handled
<xref target="RFC7530"/>
<xref target="RFC5661"/>.
As in that case, implementations that need to encode and decode
these nominally opaque entities need to use the protocol description
to determine the actual XDR representation that underlays the
items described as opaque.
</t>
<t>
The transport stream is not represented as a single XDR object.
Instead, the header prefix is described by one XDR object  while
the rest of the header is described as another XDR object with
the mapping between the header type in the header prefix and the
XDR object representing the header type represented by tables
contained in this document, with additional mappings being
specifiable by a later extension document.
<vspace blankLines="1"/>
This situation is similar to that in which RPC message headers
contain program and procedure numbers, so that the XDR for
those request and replies can be used to encode and decode the
associated messages without requiring that all be present in a
single XDR specification.
As in that case, implementations need to use the header specification
to select the appropriate XDR-generated code to be used
in message processing.
</t>
<t>
The relationship between the transport stream and the payload
stream is not specified in the XDR itself,
although comments within the XDR text make clear
where transported messages, described by their own XDR, need to appear.
Such data by its nature is opaque to the transport,
although its form differs XDR opaque arrays.
<vspace blankLines="1"/>
Potential extensions allowing continuation of RPC messages
across transport message boundaries will require that message
assembly facilities, not specifiable within XDR, also be part
of transport implementations.
</t>
</list>
</t>
<t>
To summarize, the role of XDR in this specification
is more limited than for protocols which are themselves XDR programs,
where the totality of the protocol is expressible within the
XDR paradigm established for that purpose.
This more limited role reflects the fact that XDR lacks facilities
to represent the embedding of transported material
within the transport framework.
In addition, the need to cleanly accommodate extensions
has meant that those using rpcgen in their applications
need to take a more active role in providing the facilities that
cannot be expressed within XDR.
</t>
</section>

</section>

<section
 anchor="section:e914de0a-05f3-4e14-a067-fb49a4f9b0ad"
 title="RPC Bind Parameters">
<t>
In setting up a new RDMA connection,
the first action by an RPC client is to obtain a transport address
for the RPC server.
The means used to obtain this address
and to open an RDMA connection is dependent
on the type of RDMA transport,
and is the responsibility of each RPC protocol binding
and its local implementation.
</t>
<t>
RPC services normally register with a portmap or rpcbind service
<xref target="RFC1833"/>,
which associates an RPC Program number with a service address.
This policy is no different with RDMA transports.
However, a different and distinct service address (port number)
might sometimes be required for ULP operation with RPC-over-RDMA.
</t>
<t>
When mapped atop the iWARP transport
<xref target="RFC5040"/>
<xref target="RFC5041"/>,
which uses IP port addressing due to its layering on TCP and/or SCTP,
port mapping is trivial
and consists merely of issuing the port in the connection process.  
The NFS/RDMA protocol service
address has been assigned port 20049 by IANA, for both iWARP/TCP and
iWARP/SCTP <xref target="RFC8267"/>.
</t>
<t>
When mapped atop InfiniBand
<xref target="IBA"/>,
which uses a service endpoint naming scheme based on a Group Identifier (GID),
a translation MUST be employed.
One such translation is described in
Annexes A3 (Application Specific Identifiers),
A4 (Sockets Direct Protocol (SDP)),
and A11 (RDMA IP CM Service) of
<xref target="IBA"/>,
which is appropriate for translating IP port addressing
to the InfiniBand network.
Therefore, in this case,
IP port addressing may be readily employed by the upper layer.
</t>
<t>
When a mapping standard or convention exists
for IP ports on an RDMA interconnect,
there are several possibilities for each upper layer to consider:
<list style="symbols">
<t>
One possibility is to have the server register
its mapped IP port with the rpcbind service
under the netid (or netids) defined in
<xref target="RFC8166"/>.
An RPC-over-RDMA-aware RPC client can then
resolve its desired service to a mappable port
and proceed to connect.
This is the most flexible and compatible approach
for those upper layers that are defined to use the rpcbind service.
</t>
<t>A second possibility is to have the RPC server's portmapper
register itself on the RDMA interconnect at a "well-known" service address
(on UDP or TCP, this corresponds to port 111).
An RPC client could connect to this service address
and
use the portmap protocol to obtain a service address
in response to a program number;
e.g., an iWARP port number or an InfiniBand GID.
</t>
<t>
Alternately, the RPC client could simply connect
to the mapped well-known port for the service itself,
if it is appropriately defined.
By convention, the NFS/RDMA service,
when operating atop such an InfiniBand fabric,
uses the same 20049 assignment as for iWARP.
</t>
</list>
</t>
<t>
Historically, different RPC protocols have taken different approaches
to their port assignment.
Therefore, the specific method is left
to each RPC-over-RDMA-enabled ULB and is not addressed in this document.
</t>
<t>
<xref target="RFC8166"/>
defines two new netid values
to be used for registration of upper layers
atop iWARP
<xref target="RFC5040"/> <xref target="RFC5041"/>
and (when a suitable port translation service is available) InfiniBand
<xref target="IBA"/>.
Additional RDMA-capable networks MAY define their own netids,
or if they provide a port translation,
they MAY share the one defined in
<xref target="RFC8166"/>.
</t>
</section>

<!-- RFC Editor:
Please remove the following section and the reference to RFC 7942
before this document is published.
-->

<section
 anchor="section:fd2f45e7-85fa-4863-a4cd-ea200878062f"
 title="Implementation Status">
<t>
This section records the status of known implementations of the protocol
defined by this specification at the time of posting of this Internet-Draft,
and is based on a proposal described in
<xref target="RFC7942"/>.
The description of implementations in this section is intended to
assist the IETF in its decision processes in progressing drafts to RFCs.
</t>
<t>
Please note that the listing of any individual implementation here
does not imply endorsement by the IETF.
Furthermore, no effort has been spent to verify the information presented here
that was supplied by IETF contributors.
This is not intended as, and must not be construed to be,
a catalog of available implementations or their features.
Readers are advised to note that other implementations may exist.
</t>
<t>
At this time, no known implementations of the protocol
described in this document exist.
</t>
</section>

<section
 anchor="section:912A2C09-95EC-4CB6-AA2B-2245726D9EDF"
 title="Security Considerations">

<section
 anchor="section:9e0f4573-2f3e-4758-9a9d-c5ae8f54d5f6"
 title="Memory Protection">
<t>
A primary consideration is the protection of the integrity and confidentiality
of host memory by an RPC-over-RDMA transport.
The use of an RPC-over-RDMA transport protocol MUST NOT introduce vulnerabilities
to system memory contents nor to memory owned by user processes.
</t>
<t>
It is REQUIRED that any RDMA provider used for RPC transport
be conformant to the requirements of
<xref target="RFC5042"/>
in order to satisfy these protections.
These protections are provided by the RDMA layer specifications,
and in particular, their security models.
</t>

<section
 anchor="section:2ca69bb1-1d40-4226-a375-face55cf0108"
 title="Protection Domains">
<t>
The use of Protection Domains to limit the exposure of memory regions
to a single connection is critical.
Any attempt by an endpoint not participating in that connection to reuse memory handles
needs to result in immediate failure of that connection.
Because ULP security mechanisms rely on this aspect of Reliable connected behavior,
strong authentication of remote endpoints is recommended.
</t>
</section>

<section
 anchor="section:35460fd7-b8e5-4c95-901c-fbe827b61966"
 title="Handle (STag) Predictability">
<t>
Unpredictable memory handles should be used
for any operation requiring advertised memory regions.
Advertising a continuously registered memory region
allows a remote host to read or write to that region even
when an RPC involving that memory is not under way.
Therefore, implementations should avoid advertising persistently
registered memory.
</t>
</section>

<section
 anchor="section:958d6c42-3fa3-4368-9ea9-2f43b8795bcb"
 title="Memory Protection">
<t>
Requesters should register memory regions for remote access
only when they are about to be the target of an RPC operation that involves an RDMA Read or Write.
</t>
<t>
Registered memory regions should be invalidated
as soon as related RPC operations are complete.
Invalidation and DMA unmapping of memory regions
should be complete before message integrity checking
is done and before the RPC consumer is allowed to continue
execution and use or alter the contents of a memory region.
</t>
<t>
An RPC transaction on a Requester might be
terminated before a reply arrives if the RPC consumer exits unexpectedly
(for example, it is signaled or a segmentation fault occurs).
When an RPC terminates abnormally, memory regions associated with that RPC should be invalidated appropriately
before the regions are released to be reused for other purposes on the Requester.
</t>
</section>

<section
 anchor="section:71519a0b-0458-4ae8-912d-2f09a968ab09"
 title="Denial of Service">
<t>
A detailed discussion of denial-of-service exposures
that can result from the use of an RDMA transport is found
in Section 6.4 of
<xref target="RFC5042"/>.
</t>
<t>
A Responder is not obliged to pull Read chunks that are unreasonably large.
The Responder can use an RDMA2_ERROR response to terminate RPCs with unreadable Read chunks.
If a Responder transmits more data than a Requester is prepared to receive in a Write or Reply chunk,
the RDMA Network Interface Cards (RNICs) typically terminate the connection.
For further discussion, see
<xref target="section:b1d23e5c-31df-483f-adb7-25430b5de38d"/>.
Such repeated chunk errors can deny service
to other users sharing the connection from the errant Requester.
</t>
<t>
An RPC-over-RDMA transport implementation is not responsible
for throttling the RPC request rate,
other than to keep the number of concurrent RPC transactions
at or under the number of credits granted per connection.
This is explained in <xref target="section:45c67eb8-8dc6-47c3-8555-14270f1514bF"/>.
A sender can trigger a self denial of service by exceeding the credit grant repeatedly.
</t>
<t>
When an RPC has been canceled due to a signal or premature exit of an application process,
a Requester typically invalidates the RPC's Write and Reply chunks.
Invalidation prevents the subsequent arrival of the Responder's reply
from altering the memory regions associated with those chunks after the memory has been reused.
</t>
<t>On the Requester, a malfunctioning application or a malicious user can create a situation
where RPCs are continuously initiated and then aborted,
resulting in Responder replies that terminate the underlying RPC-over-RDMA connection repeatedly.
Such situations can deny service to other users sharing the connection from that Requester.
</t>
</section>

</section>

<section
 anchor="section:4b069dfd-7532-4b9b-a9c9-1f0e8ee0d2fC"
 title="RPC Message Security">
<t>
ONC RPC provides cryptographic security via the RPCSEC_GSS framework
<xref target="RFC7861"/>.
RPCSEC_GSS implements
message authentication (rpc_gss_svc_none),
per-message integrity checking (rpc_gss_svc_integrity),
and
per-message confidentiality (rpc_gss_svc_privacy)
in the layer above the RPC-over-RDMA transport.
The latter two services require significant computation
and movement of data on each endpoint host.
Some performance benefits enabled by RDMA transports can be lost.
</t>

<section
 anchor="section:ca56ff24-b218-455a-9faf-c8f7c17bf26c"
 title="RPC-over-RDMA Protection at Lower Layers">
<t>
For any RPC transport, utilizing RPCSEC_GSS integrity or privacy services
has performance implications.
Protection below the RPC transport is often more appropriate
in performance-sensitive deployments,
especially if it, too, can be offloaded.
Certain configurations of IPsec can be co-located in RDMA hardware,
for example, without change to RDMA consumers and little loss of data movement efficiency.
Such arrangements can also provide a higher degree of privacy
by hiding endpoint identity or altering the frequency at which messages are exchanged,
at a performance cost.
</t>
<t>
The use of protection in a lower layer MAY be negotiated
through the use of an RPCSEC_GSS security flavor defined in
<xref target="RFC7861"/>
in conjunction with the Channel Binding mechanism
<xref target="RFC5056"/>
and IPsec Channel Connection Latching
<xref target="RFC5660"/>.
Use of such mechanisms is REQUIRED where integrity or confidentiality
is desired and where efficiency is required.
</t>
</section>

<section
 anchor="section:61deca2d-94c4-4fd4-be7a-59ae3a77c9a2"
 title="RPCSEC_GSS on RPC-over-RDMA Transports">
<t>
Not all RDMA devices and fabrics support the above protection mechanisms.
Also, per-message authentication is still required
on NFS clients where multiple users access NFS files.
In these cases, RPCSEC_GSS can protect NFS traffic conveyed on RPC-over-RDMA connections.
</t>
<t>
RPCSEC_GSS extends the ONC RPC protocol without changing the format of RPC messages.
By observing the conventions described in this section,
an RPC-over-RDMA transport can convey RPCSEC_GSS-protected RPC messages interoperably.
</t>
<t>
As part of the ONC RPC protocol,
protocol elements of RPCSEC_GSS that appear in the Payload stream of an RPC-over-RDMA message
(such as control messages exchanged as part of
establishing
or
destroying a security context
or
data items that are part of RPCSEC_GSS authentication material)
MUST NOT be reduced.
</t>

<section
 anchor="section:17014ff8-d5ef-4db8-bbb5-337a07cd66e2"
 title="RPCSEC_GSS Context Negotiation">
<t>
Some NFS client implementations use a separate connection
to establish a Generic Security Service (GSS) context for NFS operation.
Such clients use TCP and the standard NFS port (2049) for context establishment.
To enable the use of RPCSEC_GSS with NFS/RDMA,
an NFS server MUST also provide a TCP-based NFS service on port 2049.
</t>
</section>

<section
 anchor="section:77AA4781-E811-4B0D-8704-F96CCD4888DF"
 title="RPC-over-RDMA with RPCSEC_GSS Authentication">
<t>
The RPCSEC_GSS authentication service has no impact on the DDP-eligibility
of data items in a ULP.
</t>
<t>
However, RPCSEC_GSS authentication material appearing in an RPC message header
can be larger than, say, an AUTH_SYS authenticator.
In particular, when an RPCSEC_GSS pseudoflavor is in use,
a Requester needs to accommodate a larger RPC credential
when marshaling RPC Call messages and needs to provide for a maximum size RPCSEC_GSS verifier
when allocating reply buffers and Reply chunks.
</t>
<t>RPC messages, and thus Payload streams,
are made larger as a result.
ULP operations that fit in a Simple Format message when a simpler form of authentication is in use
might need to be reduced or conveyed via a Special Format message
when RPCSEC_GSS authentication is in use.
It is more likely that a Requester provides both a Read list and a Reply chunk
in the same RPC-over-RDMA Transport header to convey a Special Format Call
and provision a receptacle for a Special Format Reply.
</t>
<t>
In addition to this cost, the XDR encoding and decoding
of each RPC message using RPCSEC_GSS authentication
requires host compute resources to construct the GSS verifier.
</t>
</section>

<section
 anchor="section:4574ad52-fe73-4679-a808-ae3612c60f24"
 title="RPC-over-RDMA with RPCSEC_GSS Integrity or Privacy">
<t>
The RPCSEC_GSS integrity service enables endpoints
to detect modification of RPC messages in flight.
The RPCSEC_GSS privacy service prevents all but the intended recipient
from viewing the cleartext content of RPC arguments and results.
RPCSEC_GSS integrity and privacy services are end-to-end.
They protect RPC arguments and results from application to server endpoint, and back.
</t>
<t>
The RPCSEC_GSS integrity and encryption services operate
on whole RPC messages after they have been XDR encoded for transmit,
and before they have been XDR decoded after receipt.
Both sender and receiver endpoints use intermediate buffers
to prevent exposure of encrypted data or unverified cleartext data to RPC consumers.
After verification, encryption, and message wrapping has been performed,
the transport layer MAY use RDMA data transfer between these intermediate buffers.
</t>
<t>
The process of reducing a DDP-eligible data item removes
the data item
and
its XDR padding
from the encoded Payload stream.
XDR padding of a reduced data item is not transferred in a normal RPC-over-RDMA message.
After reduction, the Payload stream contains fewer octets than the whole XDR stream did beforehand.
XDR padding octets are often zero bytes, but they don't have to be.
Thus, reducing DDP-eligible items affects the result of message integrity verification or encryption.
</t>
<t>
Therefore, a sender MUST NOT reduce a Payload stream
when RPCSEC_GSS integrity or encryption services are in use.
Effectively, no data item is DDP-eligible in this situation,
and Formats with chunks cannot be used.
In this mode, an RPC-over-RDMA transport operates
in the same manner as a transport that does not support DDP.
</t>
<t>
When an RPCSEC_GSS integrity or privacy service is in use,
a Requester provides both a Read list and a Reply chunk in the same RPC-over-RDMA header
to convey a Special Format Call and provision a receptacle for a Special Format Reply.
</t>
</section>

<section
 anchor="section:0cddd345-064a-4c96-b251-17afce70219f"
 title="Protecting RPC-over-RDMA Transport Headers">
<t>
Like the base fields in an ONC RPC message (XID, call direction, and so on),
the contents of an RPC-over-RDMA message's Transport stream
are not protected by RPCSEC_GSS.
This exposes XIDs, connection credit limits, and chunk lists
(but not the content of the data items they refer to)
to malicious behavior,
which could redirect data that is transferred by the RPC-over-RDMA message,
result in spurious retransmits, or trigger connection loss.
</t>
<t>
In particular, if an attacker alters
the information contained in the chunk lists of an RPC-over-RDMA Transport header,
data contained in those chunks can be redirected
to other registered memory regions on Requesters.
An attacker might alter the arguments of RDMA Read and RDMA Write operations
on the wire to similar effect.
If such alterations occur, the use of RPCSEC_GSS integrity or privacy services
enable a Requester to detect unexpected material in a received RPC message.
</t>
<t>
Encryption at lower layers, as described in
<xref target="section:ca56ff24-b218-455a-9faf-c8f7c17bf26c"/>
protects the content of the Transport stream.
To address attacks on RDMA protocols themselves,
RDMA transport implementations should conform to
<xref target="RFC5042"/>.
</t>
</section>

</section>

</section>

<section
 anchor="section:3b0e673b-98d7-436d-bd6f-180180503df6"
 title="Transport Properties">
<t>
Like other fields that appear in each RPC-over-RDMA header,
property information is sent in the clear on the fabric
with no integrity protection, making it vulnerable to
man-in-the-middle attacks.
</t>
<t>
For example, if a man-in-the-middle were to change the value of
the Receive buffer size or the Requester Remote Invalidation boolean,
it could reduce connection performance or trigger loss of connection.
Repeated connection loss can impact performance or even prevent a
new connection from being established. Recourse is to deploy on a
private network or use link-layer encryption.
</t>
</section>

<section
 anchor="section:c85be87e-4f2b-4caf-8ae5-acdaa972a9f9"
 title="Host Authentication">
<t>
Wherein we use the relevant sections of
<xref target="RFC3552"/>
to analyze the addition of host authentication to this
RPC-over-RDMA transport.
</t>
<t>
The authors refer readers to Appendix C of
<xref target="RFC8446"/> for information on how to
design and test a secure authentication handshake implementation.
</t>
</section>

</section>

<section
 anchor="section:d235c884-6463-411f-ba34-6bcc82ab7a9f"
 title="IANA Considerations">
<t>
The RPC-over-RDMA family of transports have been assigned RPC netids
by
<xref target="RFC8166"/>.
A netid is an rpcbind
<xref target="RFC1833"/>
string used to identify the underlying protocol
in order for RPC to select appropriate transport framing
and the format of the service addresses and ports.
</t>
<t>The following netid registry strings are already defined for this purpose:
<figure align="left">
<artwork xml:space="preserve" align="left">
   NC_RDMA "rdma"
   NC_RDMA6 "rdma6"
</artwork>
</figure>
</t>
<t>
The "rdma" netid is to be used when IPv4 addressing is employed by the underlying transport,
and "rdma6" when IPv6 addressing is employed.
The netid assignment policy and registry are defined in
<xref target="RFC5665"/>.
The current document does not alter these netid assignments.
</t>
<t>
These netids MAY be used for any RDMA network that satisfies the
requirements of
<xref target="section:6903045e-bd1c-4e12-bf96-6b534989f46A"/>
and that is able to identify service endpoints using IP port addressing,
possibly through use of a translation service as described in
<xref target="section:e914de0a-05f3-4e14-a067-fb49a4f9b0ad"/>.
</t>
</section>

</middle>

<back>

<references
 title="Normative References">
<?rfc include="reference.RFC.1833.xml"?>
<?rfc include="reference.RFC.2119.xml"?>
<?rfc include="reference.RFC.4506.xml"?>
<?rfc include="reference.RFC.5042.xml"?>
<?rfc include="reference.RFC.5056.xml"?>
<?rfc include="reference.RFC.5280.xml"?>
<?rfc include="reference.RFC.5531.xml"?>
<?rfc include="reference.RFC.5660.xml"?>
<?rfc include="reference.RFC.5665.xml"?>
<?rfc include="reference.RFC.6125.xml"?>
<?rfc include="reference.RFC.7861.xml"?>
<?rfc include="reference.RFC.7942.xml"?>
<?rfc include="reference.RFC.8166.xml"?>
<?rfc include="reference.RFC.8174.xml"?>
<?rfc include="reference.RFC.8267.xml"?>
<?rfc include="reference.RFC.8446.xml"?>
</references>

<references
 title="Informative References">

<reference
 anchor="IBA">
<front>
<title>InfiniBand Architecture Specification Volume 1</title>
<author>
<organization>InfiniBand Trade Association</organization>
</author>
<date month="March" year="2015"/>
</front>
<seriesInfo name="Release" value="1.3"/>
<annotation>
Available from https://www.infinibandta.org/
</annotation>
</reference>

<reference
 anchor="CBFC">
<front>
<title>
Credit-Based Flow Control for ATM Networks: Credit Update Protocol, Adaptive Credit Allocation, and Statistical Multiplexing
</title>
<author initials="H.T." surname="Kung">
<organization>Division of Applied Sciences, Harvard University</organization>
<address>
<postal>
<street>29 Oxford Street</street>
<city>Cambridge</city>
<region>MA</region>
<code>02138</code>
<country>United States of America</country>
</postal>
</address>
</author>
<author initials="T." surname="Blackwell">
<organization>Division of Applied Sciences, Harvard University</organization>
<address>
<postal>
<street>29 Oxford Street</street>
<city>Cambridge</city>
<region>MA</region>
<code>02138</code>
<country>United States of America</country>
</postal>
</address>
</author>
<author initials="A." surname="Chapman">
<organization>Bell-Northern Research</organization>
<address>
<postal>
<street>P.O.Box 3511, Station C</street>
<city>Ottawa</city>
<region>Ontario</region>
<code>KIY 4H7</code>
<country>Canada</country>
</postal>
</address>
</author>
<date month="August" year="1994"/>
</front>
<seriesInfo name="Proc." value="ACM SIGCOMM '94 Symposium on Communications Architectures, Protocols and Applications, pp. 101-114."/>
</reference>

<?rfc include="reference.RFC.0768.xml"?>
<?rfc include="reference.RFC.0793.xml"?>
<?rfc include="reference.RFC.1094.xml"?>
<?rfc include="reference.RFC.1813.xml"?>
<?rfc include="reference.RFC.3552.xml"?>
<?rfc include="reference.RFC.5040.xml"?>
<?rfc include="reference.RFC.5041.xml"?>
<?rfc include="reference.RFC.5532.xml"?>
<?rfc include="reference.RFC.5661.xml"?>
<?rfc include="reference.RFC.5662.xml"?>
<?rfc include="reference.RFC.7530.xml"?>
<?rfc include="reference.RFC.8167.xml"?>
<?rfc include="reference.RFC.8178.xml"?>
</references>

<section
 anchor="section:9e003b83-66b5-43d7-b9ef-0f271c8d301b"
 title="ULB Specifications">
<t>
An Upper-Layer Protocol (ULP) is typically defined
independently of any particular RPC transport.
An Upper-Layer Binding (ULB) specification provides guidance
that helps the ULP interoperate correctly and efficiently
over a particular transport.
For RPC-over-RDMA version 2, a ULB may provide:
<list style="symbols">
<t>
A taxonomy of XDR data items that are eligible for DDP
</t>
<t>
Constraints on which upper-layer procedures may be reduced and on
how many chunks may appear in a single RPC request
</t>
<t>
A method for determining the maximum size of the reply Payload
stream for all procedures in the ULP
</t>
<t>
An rpcbind port assignment for operation of the RPC Program and
Version on an RPC-over-RDMA transport
</t>
</list>
Each RPC Program and Version tuple that utilizes RPC-over-RDMA
version 2 needs to have a ULB specification.
</t>

<section
 anchor="section:2f2b32a4-d78a-45f0-b6a3-fa0e2d34a97b"
 title="DDP-Eligibility">
<t>
An ULB designates some XDR data items as eligible for DDP.
As an RPC-over-RDMA message is formed,
DDP-eligible data items can be removed from the Payload stream
and placed directly in the receiver's memory.
An XDR data item should be considered for DDP-eligibility if there is
a clear benefit to moving the contents of the item directly from the
sender's memory to the receiver's memory.
</t>
<t>
Criteria for DDP-eligibility include:
<list style="symbols">
<t>
The XDR data item is frequently sent or received, and its size is
often much larger than typical inline thresholds.
</t>
<t>
If the XDR data item is a result, its maximum size must be
predictable in advance by the requester.
</t>
<t>
Transport-level processing of the XDR data item is not needed.
For example, the data item is an opaque byte array, which requires
no XDR encoding and decoding of its content.
</t>
<t>
The content of the XDR data item is sensitive to address alignment.
For example, a data copy operation would be required
on the receiver to enable the message to be parsed correctly,
or
to enable the data item to be accessed.
</t>
<t>
The XDR data item does not contain DDP-eligible data items.
</t>
</list>
</t>
<t>
In addition to defining the set of data items that are DDP-eligible,
a ULB may also limit the use of chunks to particular upper-layer
procedures.
If more than one data item in a procedure is DDP-eligible,
the ULB may also limit the number of chunks that a requester
can provide for a particular upper-layer procedure.
</t>
<t>
Senders MUST NOT reduce data items that are not DDP-eligible.
Such data items MAY, however, be moved as part of a Position Zero Read
chunk or a Reply chunk.
</t>
<t>
The programming interface by which an upper-layer implementation
indicates the DDP-eligibility of a data item to the RPC transport is
not described by this specification.
The only requirements are that
the receiver can re-assemble the transmitted RPC-over-RDMA message
into a valid XDR stream, and that DDP-eligibility rules specified
by the ULB are respected.
</t>
<t>
There is no provision to express DDP-eligibility within the XDR
language.
The only definitive specification of DDP-eligibility is a ULB.
</t>
<t>
In general, a DDP-eligibility violation occurs when:
<list style="symbols">
<t>
A requester reduces a non-DDP-eligible argument data item.
The Responder MUST NOT process this RPC Call message and MUST report
the violation as described in
<xref target="section:b1d23e5c-31df-483f-adb7-25430b5de38d"/>.
</t>
<t>
A Responder reduces a non-DDP-eligible result data item.
The requester MUST terminate the pending RPC transaction
and report an appropriate permanent error to the RPC consumer.
</t>
<t>
A Responder does not reduce a DDP-eligible result data item into
an available Write chunk.
The requester MUST terminate the pending RPC transaction
and report an appropriate permanent error to the RPC consumer.
</t>
</list>
</t>
</section>

<section
 anchor="section:a3f15fe3-2677-4d94-adb5-afa80c7e197a"
 title="Maximum Reply Size">
<t>
When expecting small and moderately-sized Replies,
a requester should typically rely on Message Continuation
rather than
provisioning a Reply chunk.
For each ULP procedure where there is no clear Reply size maximum
and the maximum can be large,
the ULB should specify a dependable means for determining the maximum Reply size.
</t>
</section>

<section
 anchor="section:e568723e-3dd8-4a54-9401-92ae4f98f88f"
 title="Additional Considerations">
<t>
There may be other details provided in a ULB.
<list style="symbols">
<t>
An ULB may recommend inline threshold values or other transport-related
parameters for RPC-over-RDMA version 2 connections bearing that ULP.
</t>
<t>
An ULP may provide a means to communicate these transport-related
parameters between peers.
Note that RPC-over-RDMA version 2 does not specify any mechanism
for changing any transport-related parameter after a connection
has been established and the initial transport properties
have been exchanged.
</t>
<t>
Multiple ULPs may share a single RPC-over-RDMA version 2
connection when their ULBs allow the use of RPC-over-RDMA version 2
and the rpcbind port assignments for the Protocols allow
connection sharing.
In this case, the same transport parameters (such as inline threshold)
apply to all Protocols using that connection.
</t>
</list>
Each ULB needs to be designed to allow correct interoperation without
regard to the transport parameters actually in use.
Furthermore, implementations of ULPs must be designed
to interoperate correctly
regardless of the connection parameters in effect on a connection.
</t>
</section>

<section
 anchor="section:db58c83f-091c-481e-ba7c-a0246d1c475b"
 title="ULP Extensions">
<t>
An RPC Program and Version tuple may be extensible.
For instance, there may be a minor versioning scheme
that is not reflected in the RPC version number,
or
the ULP may allow additional features to be
specified after the original RPC Program specification was ratified.
ULBs are provided for interoperable RPC Programs and Versions by
extending existing ULBs to reflect the changes made necessary by each
addition to the existing XDR.
</t>
</section>

</section>

<section
 anchor="section:84e1ffc4-d916-4eb4-9fd8-a8218d084503"
 title="Extending the Version 2 Protocol">
<t>
This Appendix is not addressed to protocol implementers,
but rather to authors of documents that intend to extend
the protocol described earlier in this document.
</t>
<t>
Subsequent RPC-over-RDMA versions are free to change the protocol
in any way they choose as long as they leave unchanged those fields
identified as "fixed for all versions" in Section 4.2.1 of
<xref target="RFC8166"/>.
</t>
<t>
Such changes might involve deletion or major re-organization of
existing transport headers.
However, the need for interoperability between adjacent versions
will often limit the scope of changes that can be made in a single version.
</t>
<t>
In some cases it may prove desirable to transition to a new version
by using the extension features described for use with RPC-over-RDMA version 2,
by continuing the same basic extension model but allowing header types
and properties that were OPTIONAL in one version to become REQUIRED
in the subsequent version.
</t>
<t>
RPC-over-RDMA version 2 is designed to be extensible
in a way that enables the addition of OPTIONAL features
that may subsequently be converted to REQUIRED status
in a future protocol version.
The protocol may be extended by Standards Track documents
in a way analogous to that provided for Network File
System Version 4 as described in
<xref target="RFC8178"/>.
</t>
<t>
This form of extensibility enables limited extensions
to the base RPC-over-RDMA version 2 protocol presented
in this document so that new optional capabilities
can be introduced without a protocol version change,
while maintaining robust interoperability
with existing RPC-over-RDMA version 2 implementations.
The design allows extensions to be
defined, including the definition of new protocol elements, without
requiring modification or recompilation of the existing XDR.
</t>
<t>
A Standards Track document introduces each set of such protocol elements.
Together these elements are considered an OPTIONAL feature.
Each implementation is either aware of all the protocol
elements introduced by that feature or is aware of none of them.
</t>
<t>
Documents describing extensions to RPC-over-RDMA version 2 should
contain:
<list style="symbols">
<t>
An explanation of the purpose and use of each new protocol element added.
</t>
<t>
An XDR description including all of the new protocol elements,
and a script to extract it.
</t>
<t>
A description of interactions with existing extensions.
<vspace blankLines="1"/>
This includes possible requirements of other OPTIONAL features
to be present for new protocol elements to work,
or that a particular level of support
for an OPTIONAL facility is required for the new extension to work.
</t>
</list>
</t>
<t>
Implementers combine the XDR descriptions of the new features they
intend to use with the XDR description of the base protocol in this
document.
This may be necessary to create a valid XDR input file
because extensions are free to use XDR types defined in the base
protocol, and later extensions may use types defined by earlier
extensions.
</t>
<t>
The XDR description for the RPC-over-RDMA version 2 base protocol
combined with that for any selected extensions
should provide an adequate human-readable description
of the extended protocol.
</t>
<t>
The base protocol specified in this document may be extended within
RPC-over-RDMA version 2 in two ways:
<list style="symbols">
<t>
New OPTIONAL transport header types may be introduced by later
Standards Track documents.
Such transport header types will be documented as described in
<xref target="section:d4650151-40f0-4e85-8755-02c38cf8f444"/>.
</t>
<t>
New OPTIONAL transport properties may be defined in later
Standards Track documents.
Such transport properties will be documented as described in
<xref target="section:a355adad-f03b-41a6-94a8-4128b10301bb"/>.
</t>
</list>
</t>
<t>
The following sorts of ancillary  protocol elements may be added
to the protocol to support the addition of new transport properties
and header types.
<list style="symbols">
<t>
New error codes may be created as described in
<xref target="section:c2f5e937-d612-4e3a-a380-e0b15261f6a0"/>.
</t>
<t>
New flags to use within the rdma_flags field may be created as described in
<xref target="section:55a1749d-702c-48e4-b78c-0db392692c09"/>.
</t>
</list>
</t>
<t>
New capabilities can be proposed and developed independently of each other,
and implementers can choose among them.
This makes it straightforward to create and document experimental features
and then bring them through the standards process.
</t>

<section
 anchor="section:d4650151-40f0-4e85-8755-02c38cf8f444"
 title="Adding New Header Types to RPC-over-RDMA Version 2">
<t>
New transport header types are to defined in a manner similar to
the way existing ones are described in Sections
<xref target="section:9af0d451-2ef3-454f-adb9-827664ccc39c"
 pageno="false"
 format="counter"/>
through
<xref target="section:07e8c178-62df-46a7-a57e-dcf107821d93"
 pageno="false"
 format="counter"/>.
Specifically what is needed is:
<list style="symbols">
<t>
A description of the function and use of the new header type.
</t>
<t>
A complete XDR description of the new header type including a description
of the use of all fields within the header.
</t>
<t>
A description of how errors are reported, including the definition
of a mechanism for reporting errors when the error is outside the
available choices already available in the base protocol or in
other existing extensions.
</t>
<t>
An indication of whether a Payload stream must be present,
and a description of its contents and how such payload streams
are used to construct RPC messages for processing.
</t>
</list>
</t>
<t>
In addition, there needs to be additional documentation that is made
necessary due to the Optional status of new transport header types.
<list style="symbols">
<t>
Information about constraints on support for the new header types should
be provided.
For example, if support for one header type is implied
or foreclosed by another one,
this needs to be documented.
</t>
<t>
A preferred method by which a sender should determine whether the peer
supports a particular header type needs to be provided.
While it is always possible for a send a test invocation
of a particular header type to see if support is available,
when more efficient means are available
(e.g. the value of a transport property,
this should be noted.
</t>
</list>
</t>
</section>

<section
 anchor="section:55a1749d-702c-48e4-b78c-0db392692c09"
 title="Adding New Header Flags to the Protocol">
<t>
New flag bits are to defined in a manner similar to
the way existing ones are described in Sections
<xref target="section:4e23391c-b6a8-46b1-a399-c86f754b51bc"
 pageno="false"
 format="counter"/>
and
<xref target="section:fb299673-74f3-4f01-adb5-26a02ccd679f"
 pageno="false"
 format="counter"/>.
Each new flag definition should include:
<list style="symbols">
<t>
An XDR description of the new flag.
</t>
<t>
A description of the function and use of the new flag.
</t>
<t>
An indication for which header types the flag value is meaningful
and
for which header types it is an error to set the flag or to leave it unset.
</t>
<t>
A means to determine whether receivers are prepared
to receive transport headers with the new flag set.
</t>
</list>
</t>
<t>
In addition, there needs to be additional documentation that is made
necessary due to the Optional status of new transport header types.
<list style="symbols">
<t>
Information about constraints on support for the new flags should
be provided.
For example, if support for one flag is implied
or foreclosed by another one,
this needs to be documented.
</t>
</list>
</t>
</section>

<section
 anchor="section:a355adad-f03b-41a6-94a8-4128b10301bb"
 title="Adding New Transport properties to the Protocol">
<t>
The set of transport properties is designed to be extensible.
As a result, once new properties are defined in standards track documents,
the operations defined in this document may reference these new
transport properties, as well as the ones described in this document.
</t>
<t>
A standards track document defining a new transport property should
include the following information paralleling that provided in this
document for the transport properties defined herein.
<list style="symbols">
<t>
The rpcrdma2_propid value used to identify this property.
</t>
<t>
The XDR typedef specifying the form in which the property value is
communicated.
</t>
<t>
A description of the transport property that is communicated by
the sender of RDMA2_CONNPROP.
</t>
<t>
An explanation of how this knowledge could be used by the
peer receiving this information.
</t>
</list>
</t>
<t>
The definition of transport property structures is such as to make it
easy to assign unique values.
There is no requirement that a continuous set of values be used and
implementations should not rely on all such values being small integers.
A unique value should be selected when the defining document is first
published as an internet draft.
When the document becomes a standards track document,
the working group should ensure that:
<list style="symbols">
<t>
rpcrdma2_propid values specified in the document do not conflict
with those currently assigned or in use by other pending working
group documents defining transport properties.
</t>
<t>
rpcrdma2_propid values specified in the document do not conflict
with the range reserved for experimental use, as defined in
Section 8.2.
</t>
</list>
</t>
<t>
Documents defining new properties fall into a number of categories.
<list style="symbols">
<t>
Those defining new properties and explaining (only) how they
affect use of existing message types.
</t>
<t>
Those defining new OPTIONAL message types and new properties
applicable to the operation of those new message types.
</t>
<t>
Those defining new OPTIONAL message types and new properties
applicable both to new and existing message types.
</t>
</list>
</t>
<t>
When additional transport properties are proposed, the review of the
associated standards track document should deal with possible
security issues raised by those new transport properties.
</t>
</section>

<section
 anchor="section:c2f5e937-d612-4e3a-a380-e0b15261f6a0"
 title="Adding New Error Codes to the Protocol">
<t>
New error codes to be returned when using new header types
may be introduced in the same Standards Track document
that defines the new header type.
Cases in which a new error code is to be returned
by an existing header type can be accommodated
by defining the new error code
in the same Standards Track document that defines
the new transport property.
</t>
<t>
For error codes that do not require that additional error
information be returned with them,
the existing RDMA_ERR2 header can be used to report the new error.
The new error code is set as the value of rdma_err with
the result that the default switch arm of the rpcrdma2_error
(i.e. void) is selected.
</t>
<t>
For error codes that do require the return of additional error-related
information together with the error, a new header type should be defined
for the purpose of returning the error together with needed additional
information.
It should be documented just like any other new header type.
</t>
<t>
When a new header type is sent, the sender needs to be prepared to accept
header types necessary to report associated errors.
</t>
</section>

</section>

<section
 anchor="section:c2574344-5aec-427d-a5ed-048d7fcc0d95"
 title="Differences from the RPC-over-RDMA Version 1 Protocol">
<t>
This section describes the substantive changes made in
RPC-over-RDMA version 2.
</t>

<section
 anchor="section:d945b9f0-0666-4db7-9126-be57cf7b5f4f"
 title="Relationship to the RPC-over-RDMA Version 1 XDR Definition">
<t>
There are a number of structural XDR changes whose goal
is to enable within-version protocol extensibility.
</t>
<t>
The RPC-over-RDMA version 1 transport header is defined as a single XDR object,
with an RPC message proper potentially following it.
In RPC-over-RDMA version 2, as described in
<xref target="section:417e749e-efec-455f-aae7-12535b9ee8dc"/>
there are separate XDR definitions of the transport header prefix
(see
<xref target="section:2d1735f0-c465-43c6-9c18-3da6b7979862"/>
which specifies the transport header type to be used,
and the specific transport header, defined within one of the subsections of
<xref target="section:eef6a22e-2633-44a2-a8f0-821fec8bf824"/>).
This is similar to the way that an RPC message consists of
an RPC header (defined in
<xref target="RFC5531"/>)
and an RPC request or reply,
defined by the Upper-Layer protocol being conveyed.
</t>
<t>
As a new version of the RPC-over-RDMA transport protocol,
RPC-over-RDMA version 2 exists within the versioning rules defined in
<xref target="RFC8166"/>.
In particular, it maintains the first four words of the protocol header
as sent and received,
as specified in Section 4.2 of
<xref target="RFC8166"/>,
even though, as explained in
<xref target="section:e21d4f74-b536-47f2-9d07-c03a27a20de4"/>
of this document,
the XDR definition of those words is structured differently.
</t>
<t>
Although each of the first four words retains its semantic function,
there are important differences of field interpretation, besides the fact
that the words have different names and different roles with the XDR
constrict of they are parts.
<list style="symbols">
<t>
The first word of the header, previously the rdma_xid field,
retains the format and function that in had in
RPC-over-RDMA version 1.
Within RPC-over-RDMA version 2,
this word is the rdma_xid field of the structure rdma_start.
However, to accommodate the use of request-response
pairing of non-RPC messages and the potential use of message continuation,
it cannot be assumed that it will always have the same value
it would have had in RPC-over-RDMA version 1.
As a result, the contents of this field should not be used
without consideration of the associated protocol version identification.
</t>
<t>
The second word of the header, previously the rdma_vers field,
retains the format and function that it had in RPC-over-RDMA version 1.
Within RPC-over-RDMA version 2,
this word is the rdma_vers field of the structure rdma_start.
To clearly distinguish version 1 and version 2 messages,
senders MUST fill in the correct version (fixed after version negotiation)
and receivers MUST check that the content of the rdma_vers is correct
before using referencing any other header field.
</t>
<t>
The third word of the header, previously the rdma_credit field,
retains the size and general purpose that it had in RPC-over-RDMA version 1.
Within RPC-over-RDMA version 2,
this word is the rdma_credit field of the structure rdma_start.
</t>
<t>
The fourth word of the header,
previously the union discriminator field rdma_proc,
retains its format and general function even though
the set of valid values has changed.
The value of this field is now considered an unsigned 32-bit integer
rather than an enum.
Within RPC-over-RDMA version 2,
this word is the rdma_htype field of the structure rdma_start.
</t>
</list>
</t>
<t>
Beyond conforming to the restrictions specified in
<xref target="RFC8166"/>,
RPC-over-RDMA version 2 tightly limits the scope
of the changes made in order to ensure interoperability.
It makes no major structural changes to the protocol,
and all existing transport header types used in version 1
(as defined in
<xref target="RFC8166"/>)
are retained in version 2.
Chunks are expressed using the same on-the-wire format and
are used in the same way in both versions.
</t>
</section>

<section
 anchor="section:630314a8-1cf5-40f7-a5ad-5bc12c719233"
 title="Transport Properties">
<t>
RPC-over-RDMA version 2 provides a mechanism for
exchanging the transport's operational properties.
This mechanism allows connection endpoints to communicate the properties
of their implementation at connection setup.
The mechanism could be expanded to enable an endpoint to request changes
in properties of the other endpoint and to notify peer endpoints of
changes to properties that occur during operation.
Transport properties are described in
<xref target="section:86248e99-ca60-478a-8aff-3fb387410077"/>.
</t>
</section>

<section
 anchor="section:5a2e5ff8-0f0b-454d-b9b3-c6773cd77780"
 title="Credit Management Changes">
<t>
RPC-over-RDMA transports employ credit-based flow control
to ensure that a requester does not emit more RDMA Sends
than the responder is prepared to receive.
Section 3.3.1 of
<xref target="RFC8166"/>
explains the purpose and operation
of RPC-over-RDMA version 1 credit management in detail.
</t>
<t>
In the RPC-over-RDMA version 1 design,
each RDMA Send from a requester contains an RPC Call with a credit request,
and each RDMA Send from a responder contains an RPC Reply with a credit grant.
The credit grant implies that enough Receives have been posted on
the responder to handle the credit grant minus the number of pending
RPC transactions (the number of remaining Receive buffers might be zero).
</t>
<t>
In other words, each RPC Reply acts as an implicit ACK
for a previous RPC Call from the requester,
indicating that the responder has posted a Receive to replace
the Receive consumed by the requester's RDMA Send.
Without an RPC Reply message, the requester has no way to know
that the responder is properly prepared for subsequent RPC Calls.
</t>
<t>
Aside from being a bit of a layering violation,
there are basic (but rare) cases where this arrangement is inadequate:
<list style="symbols">
<t>
When a requester retransmits an RPC Call on the same connection
as an earlier RPC Call for the same transaction.
</t>
<t>
When a requester transmits an RPC operation that requires no reply.
</t>
<t>
When more than one RPC-over-RDMA message is needed to complete the
transaction (e.g., RDMA_DONE).
</t>
</list>
Typically, the connection must be replaced in these cases.
This resets the credit accounting mechanism but has an undesirable impact
on other ongoing RPC transactions on that connection.
</t>
<t>
Because credit management accompanies each RPC message,
there is a strict one-to-one ratio between RDMA Send and RPC message.
There are interesting use cases that might be enabled if this relationship
were more flexible:
<list style="symbols">
<t>
RPC-over-RDMA operations which do not carry an RPC message;
e.g., control plane operations.
</t>
<t>
A single RDMA Send that conveys more than one RPC message
for the purpose of interrupt mitigation.
</t>
<t>
An RPC message that is conveyed via several sequential RDMA Sends
to reduce the use of explicit RDMA operations for moderate-sized RPC messages.
</t>
<t>
An RPC transaction that needs multiple exchanges
or an odd number of RPC-over-RDMA operations
to complete.
</t>
</list>
Bi-directional RPC operation also introduces an ambiguity.
If the RPC-over-RDMA message does not carry an RPC message, then
it is not possible to determine whether the sender is a requester
or a responder, and thus whether the rdma_credit field contains
a credit request or a credit grant.
</t>
<t>
A more sophisticated credit accounting mechanism is provided in
RPC-over-RDMA version 2 in an attempt to address some of these shortcomings.
This new mechanism is detailed in
<xref target="section:45c67eb8-8dc6-47c3-8555-14270f1514bF"/>.
</t>
</section>

<section
 anchor="section:f7fb5108-58ea-4718-84be-5119a302f5f5"
 title="Inline Threshold Changes">
<t>
The term "inline threshold" is defined in Section 3.3.2 of
<xref target="RFC8166"/>.
An "inline threshold" value is the largest message size (in octets)
that can be conveyed on an RDMA connection using only RDMA Send and Receive.
Each connection has two inline threshold values: one for messages
flowing from
client-to-server (referred to as the "client-to-server inline threshold")
and one for messages flowing from server-to-client
(referred to as the "server-to-client inline threshold").
Note that
<xref target="RFC8166"/>
uses somewhat different terminology.
This is because it was written
with only forward-direction RPC transactions in mind.
</t>
<t>
A connection's inline thresholds determine when RDMA Read or
Write operations are required because the RPC message to be
sent cannot be conveyed via a single RDMA Send and Receive pair.
When an RPC message does not contain DDP-eligible data items,
a requester can prepare a Special Format Call or Reply to convey the whole
RPC message using RDMA Read or Write operations.
</t>
<t>
RDMA Read and Write operations require that each data payload
resides in a region of memory that is registered with the RNIC.
When an RPC is complete, that region is invalidated, fencing it
from the responder.
Memory registration and invalidation typically have a latency cost
that is insignificant compared to data handling costs.
When a data payload is small, however, the cost of registering and
invalidating the memory where the payload resides becomes
a relatively significant part of total RPC latency.
Therefore the most efficient operation of RPC-over-RDMA occurs
when explicit RDMA Read and Write operations are used for large payloads,
and are avoided for small payloads.
</t>
<t>
When RPC-over-RDMA version 1 was conceived, the typical size
of RPC messages that did not involve a significant data payload
was under 500 bytes.
A 1024-byte inline threshold adequately minimized the frequency
of inefficient Special Format messages.
</t>
<t>
With NFS version 4.1
<xref target="RFC5661"/>,
the increased size of NFS COMPOUND operations
resulted in RPC messages that are on average larger
and more complex than previous versions of NFS.
With 1024-byte inline thresholds, RDMA Read or Write operations
are needed for frequent operations that do not bear a data payload,
such as GETATTR and LOOKUP,
reducing the efficiency of the transport.
</t>
<t>
To reduce the need to use Special Format messages, RPC-over-RDMA
version 2 increases the default size of inline thresholds.
This also increases the maximum size of reverse-direction
RPC messages.
</t>
</section>

<section
 anchor="section:1f3a1439-702f-4309-8733-5fa0e20555f4"
 title="Message Continuation Changes">
<t>
In addition to a larger default inline threshold,
RPC-over-RDMA version 2 introduces Message Continuation.
Message Continuation is a mechanism that enables
the transmission of a data payload using more than one RDMA Send.
The purpose of Message Continuation
is to provide relief in several important cases:
<list style="symbols">
<t>
If a requester finds that it is inefficient to
convey a moderately-sized data payload using Read chunks,
the requester can use Message Continuation to send the RPC Call.
</t>
<t>
If a requester has provided insufficient Reply chunk space
for a responder to send an RPC Reply,
the responder can use Message Continuation to send the RPC Reply.
</t>
<t>
If a sender has to convey a large non-RPC data payload
(e.g, a large transport property),
the sender can use Message Continuation to avoid using registered memory.
</t>
</list>
</t>
</section>

<section
 anchor="section:16f03208-32cb-451d-90ab-a6f5f4b9e9b0"
 title="Host Authentication Changes">
<t>
For general operation of NFS on open networks,
we eventually intend to rely on RPC-on-TLS [citation needed]
to provide cryptographic authentication of the two ends of each connection.
In turn, this will improve the trustworthiness of AUTH_SYS-style user identities
that flow on TCP, which are not cryptographic.
We do not have a similar solution for RPC-over-RDMA, however.
</t>
<t>
Here, the RDMA transport layer already provides a strong guarantee of message
integrity.
On some network fabrics, IPsec can be used to protect the privacy of in-transit data,
or TLS itself could be used for transporting raw RDMA operations.
However, this is not the case for all fabrics
(e.g., InfiniBand <xref target="IBA"/>).
</t>
<t>
Thus, it is sensible to add a mechanism in the RPC-over-RDMA transport itself
for authenticating the connection peers.
This mechanism is described in
<xref target="section:5f63e1b6-8d24-453b-b18b-b98ad66f3671"/>.
And like GSS channel binding, there should also be a way to determine
when the use of host authentication is superfluous and can be avoided.
</t>
</section>

<section
 anchor="section:57c034d6-7129-4f7b-b8df-31e8bc691964"
 title="Support for Remote Invalidation">
<t>
An STag that is registered using
the FRWR mechanism in a privileged execution context
or is registered via a Memory Window in an unprivileged context
may be invalidated remotely
<xref target="RFC5040"/>.
These mechanisms are available when a requester's
RNIC supports MEM_MGT_EXTENSIONS.
</t>
<t>
For the purposes of this discussion, there are two classes of STags.
Dynamically-registered STags are used in a single RPC, then invalidated.
Persistently-registered STags live longer than one RPC.
They may persist for the life of an RPC-over-RDMA connection, or longer.
</t>
<t>
An RPC-over-RDMA requester may provide more than one STag
in one transport header.
It may provide a combination of dynamically- and
persistently-registered STags in one RPC message, or
any combination of these in a series of RPCs on the same connection.
Only dynamically-registered STags using Memory Windows
or FRWR (i.e., registered via MEM_MGT_EXTENSIONS) may be invalidated remotely.
</t>
<t>
There is no transport-level mechanism by which a responder can determine
how a requester-provided STag was registered, nor whether it is
eligible to be invalidated remotely.
A requester that mixes persistently- and dynamically-registered STags in one RPC,
or mixes them across RPCs on the same connection,
must therefore indicate which handles may be invalidated via a mechanism
provided in the Upper-Layer Protocol.
RPC-over-RDMA version 2 provides such a mechanism.
</t>
<t>
The RDMA Send With Invalidate operation is used to invalidate an
STag on a remote system.
It is available only when a responder's RNIC supports MEM_MGT_EXTENSIONS,
and must be utilized only when a requester's RNIC supports MEM_MGT_EXTENSIONS
(can receive and recognize an IETH).
</t>
<t>
Existing RPC-over-RDMA transport protocol specifications
<xref target="RFC8166"/>
<xref target="RFC8167"/>
do not forbid direct data placement in the reverse direction,
even though there is currently no Upper-Layer Protocol that
makes data items in reverse direction operations elegible
for direct data placement.
</t>
<t>
When chunks are present in a reverse direction RPC request,
Remote Invalidation allows the responder
to trigger invalidation of a requester's STags as part of sending a reply,
the same way as is done in the forward direction.
</t>
<t>
However, in the reverse direction,
the server acts as the requester,
and the client is the responder.
The server's RNIC, therefore, must support receiving an IETH,
and the server must have registered the STags
with an appropriate registration mechanism.
</t>
</section>

<section
 anchor="section:e554df42-6e82-4e28-96a0-8f9872eb476c"
 title="Error Reporting Changes">
<t>
RPC-over-RDMA version 2 expands the repertoire of errors that
may be reported by connection endpoints.
This change, which is structured to enable extensibility,
allows a peer to report overruns of specific resources
and to avoid requester retries when an error is permanent.
</t>
</section>

</section>

<section
 anchor="section:7b212a81-9c2a-4c05-891a-369cc7184585"
 title="Acknowledgments"
 numbered="no">
<t>
The authors gratefully acknowledge the work of Brent Callaghan
and Tom Talpey on the original RPC-over-RDMA version 1 specification
(RFC 5666).
The authors also wish to thank
Bill Baker, Greg Marsden, and Matt Benjamin
for their support of this work.
</t>
<t>
The XDR extraction conventions were
first described by the authors of the NFS version 4.1
XDR specification
<xref target="RFC5662"/>.
Herbert van den Bergh suggested the replacement sed
script used in this document.
</t>
<t>
Special thanks go to
Transport Area Director Magnus Westerlund,
NFSV4 Working Group Chairs Spencer Shepler
and
Brian Pawlowski,
and
NFSV4 Working Group Secretary Thomas Haynes
for their support.
</t>
</section>

</back>

</rfc>
